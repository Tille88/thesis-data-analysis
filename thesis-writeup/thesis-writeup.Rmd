---
output: 
  pdf_document:
    citation_package: natbib
    # keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    number_sections: true  
header-includes:
  -  \usepackage{hyperref}
title: "Opacity data perception support using legends when overlaid on complex map backgrounds"
---

<!-- TODO : MATCH WITH EXPECTATIONS FROM FORMATTING AND CONTENT OF START PAGE -->
<!-- TODO : **ABSTRACT** -->



\newpage
\tableofcontents 
\newpage


```{r, setup, include=FALSE}
# knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
library(mongolite)
library(stringr)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(gtsummary)
```

```{r echo=FALSE}
figure_number = 0 
```



\section{Introduction} \label{introduction}

## Data Visualization and GIS

There are clear overlapping goals between the area of data visualization - with the target visually encoding data attributes in a way that is visually pleasing, and easy for the target audience to decode - and visual representation of data on a map in a GIS-system. It may also be possible to consider visual representation of data for an end user in a GIS-system as a subfield of data visualization, and related fields such as the study of human perception, where most textbooks in these fields contains a section on geographic mapping of data (see e.g. Munzner, 2015, p. 181; Grant, 2019; Cairo 2016; Ware, 2020, p. 105, 125, 128ff).


## Opacity/transparency in GIS visualizations

Colin Ware (2020) highlights this relation between the fields of Data Visualization and GIS in his seminal work on information visualization in relation to the use of transparency as a technique to the use of transparency (alpha channel mapping/encoding) in information visualization:

> *In many visualization problems, it is desirable to present data in a layered form. This is especially common in geographic information systems (GISs). So that the contents of different layers are simultaneously visible, a useful technique is to present one layer of data transparently over another; however, there are many perceptual pitfalls in doing this. The contents of the different layers will always interfere with each other to some extent, and sometimes the two layers will fuse perceptually so that it is impossible to determine to which layer a given object belongs (ibid. p. 217).*

```{r echo=FALSE}
figure_number = figure_number + 1 
```


Ware (ibid, p. 357) also includes a figure which shows a fully opaque data encoding on top of a map layer (Figure `r figure_number`) – which depending on need can make it difficult for the end user to know the demarcations of the different categories due to occlusion.

 ![Figure `r figure_number`. Fully opaque data encoding resulting in lower layers not being visible, possible resulting in the end user having difficulties knowing the exact underlying geography.](./img/picture1.png)


```{r echo=FALSE}
figure_number = figure_number + 1 
```


The increased accessibility and ease to create maps for online usage makes different data mappings available, there are a lot of maps that make liberal use of a mix of mapping techniques, such as colour and opacity (Figure `r figure_number`) without taking into account the end-users ability to easily decode the data being visualized.
 
 ![Figure `r figure_number`. Liberal web-GIS use of multiple encodings without clear data mapping clarifications for end-user (Ee, 2020).](./img/picture2.png)


```{r echo=FALSE}
figure_number = figure_number + 1 
opacity_picker = figure_number
```


The availability and ease of using opacity mappings and other techniques are also shown by the market leader
\footnote{
”Esri builds the leading mapping and spatial analytics software for desktop, software as a service (SaaS), and enterprise applications. Esri ArcGIS products are designed to deliver location intelligence and meet digital transformation needs for organizations of all sizes. Here you can explore Esri software, apps, content, solutions, and developer tools.” - https://www.esri.com/en-us/arcgis/products/index Accessed 13 Sept 2020

”[…]our market-leading Enterprise GIS mapping software will support your work and deliver results” - https://www.esri.com/en-us/arcgis/products/arcgis-enterprise/overview Accessed 13 Sept 2020

} 
ESRI/ArcGIS examples and tutorials for their products, such as “ArcGIS online” and “ArcGIS Enterprise Map Viewer” are shown by the blog-entry “6 Easy Ways to Improve Your Maps”, where “4) Utilize Transparency” is included. The ways the transparency tool works is included as well (Figure `r opacity_picker`) does not give the user much information or consider underlying layer colors and effects on perception.

 ![Figure `r opacity_picker`. Transparency data mapping/encoding for ArcGIS products (Berry, 2016)](./img/picture3.png)

```{r echo=FALSE}
figure_number = figure_number + 1 
dark_background_example = figure_number
figure_number = figure_number + 1 
bright_background_example = figure_number
```


This mapping has a similar visual representation to the end user of the maps as shown in the usage examples from the “ArcGIS Online” API documentation ArcGIS website (Figure `r dark_background_example` and `r bright_background_example`). Figure `r dark_background_example` includes colour mapping, but the legend maps transparency into the same checkered grey/white background as for the tool shown in Figure `r opacity_picker`, making it very difficult to decode the visual values. Figure `r bright_background_example` has a single colour and a background layer corresponding roughly to the constant mid-grey and white checkered pattern.  

 ![Figure `r dark_background_example`. Opacity data mapping and legend with colour scale (ArcGIS, 2020a)](./img/picture4.png)
 
 ![Figure `r bright_background_example`. Opacity data mapping and legend (ArcGIS, 2020b)](./img/picture5.png)

This type of legend representation of opacity is in not context-aware, and remains a checkered mid-grey background no matter the background of the map. 

In static 2D maps, without interactive tools, such as tooltips – or in areas where hovering effects can not be as easily used (such as when using a tablet or smart phone), the end user have to rely on attempts to decode the visual representations with basic tools such as legends.

As will be discussed in section \ref{background}, legends have been studied, but not with focus on transparency decoding or attempts to use alternative techniques for transparency legends, such as contextualizing the legend or trying to increase the proximity of the to the encoded data. 

## Research Questions

- How is data opacity mapping perception influenced by legend display choices?
- Does alternative legend methods compared to GIS industry standard’s choice influence errors in perceived data mapping and colour choices?
- Does legend to data proximity-reduction or contextualised legend design influence perception errors for opacity data mapping?
- Can decision times and user selection behaviour be affected by legend choices?
- How are alternative legend design choices received by the subjects (subjective user acceptance testing)?



<!-- # Background -->
\section{Background} \label{background}

*TODO: INCLUDE MORE MEAT IN LITERATURE SECTION!!!*
**The background put the study in a wider perspective and gives a summary or ‘state of the art’ of the actual question. This chapter can present the literature study as a base to the problem and posed question. Already here you can present actual literature/references to research in the field of question, but this should be a presentation. Often you can use this summary of the stat of the art again the discussion of your results. Take care that you have the latest publications in the field of research topic included.**

## Previous related literature

The literature review will touch on several areas which intersect this study scope: 
i) data visualization and perception, low-level and design studies ii) transparency-focused studies iii) cartography and GIS visualization, including colour studies iv) use of legends.
These areas have certain number of overlap, but this grouping progressively moves towards the focus of this study.

i) Data Visualization and Perception
Brehmer and Munzner (2013) describes a gap between low-level tasks, and high-level tasks in the data visualization literature. 
Representative for the low-level perception-focused literature is the early studies focusing on measuring the ability of humans to correctly perceive different visual data encodings, such as position, length, angle, circle area, hue, etc. (Cleveland and McGill, 1985).
Other examples of low-level perception studies are Adelson’s (1982 and 1993) studies on perception effects from context, such as luminosity perception of a grey area depending on surrounding areas.
For the higher-level tasks, Tufte (2001) exemplifies attempts to bring these smaller encodings together to more stringently describe good data visualization practices.
Newer literature in the area commonly attempts to bridge the gap towards other scientific areas (Kim et al. 2017a, Kim et al. 2017b), incorporating mental models, uncertainty and Bayesian prior beliefs of subjects into the perception task.


ii) Transparency
Transparancy models have been used and studied extensively, with early opacity/transparency models by Fabio Metelli (1974) still being in use to this day.
TODO: rewrite, currently citing Ekroll's Transparency perception: the key to understanding simultaneous color contrast
Mitelli’s well-established additive model of perceptual transparency, a transparent layer of color L in front of a background region of color B produces a proximal color signal P which is simply a weighted mixture

<!-- P = \alpha·B+(1−\alpha)*L -->

Current transparency literature is heavily focused on computational rendering in a 3D/volume rendering context (e.g. Chan et al. 2009 and Correra & Ma, 2008).


iii) Cartography and GIS
Colour choices in a cartographic context is a large literature, with Brewer (2006) having provided colour palette tools commonly used throughout data visualization from studying colour use from a cartographic/GIS perspective.
Studies that cover transparency directly or indirectly are more rare. Jo et al. (2019) briefly covers opacity: "To scale scatterplots, several approaches have been proposed, such as adaptive opacity [15, 30, 32] and aggregation [13, 53]. However, adaptive opacity does not scale well with the number of categories since multiple categorical colors become ambiguous when blended[..]".
Another use of transparency with geospatial data in a design study setting is the 'value-by-alpha-map' described as: "A value-by-alpha map relates the alpha channel of each displayed enumeration unit to the unit's value in an equalizing variable. The value-by-alpha map solves the dilemma presented by Cartogram3. Shape and topology are both preserved perfectly because geography is maintained." (Roth et al. 2010). "Cartogram3" included as “Figure 3-1” below.
 
Figure 3-1. Value-by-alpha map comparison (Roth et al. 2010)


iv) Legends
Literature on use of legends seem highly tied to different visualization types. Recent map legend literature found have focused on interactive aspects (Midtbø, 2007) and attempts to provide guidance on the use of map legends in a “dynamic environment” (Dykes et al. 2010).
No intersection between legends and transparency either in GIS-context or elsewhere have been found in the literature review.

**Could flesh this out more with description of opacity legends in academic journals (sometimes included, not focus of studies in questions).**


# *Methodology*

**Here you present the applied methods for the collection of data, the analysis*, INCLUDE MOTIVATIONS**

## *Data Collection*


\subsubsection{Legend design types} \label{legends}
<!-- ### *Legend design types* -->

Due to the narrowness of the reasearch questions, no openly available dataset or examples was likely to exist. This resulted in extensive work to design and create a dataset and a reproducable experiment where legend types was possible to isolate as the only non-randomly changing treatment.  

The data was limited to static 2D maps and using web technologies for easy distribution. The subjects were presented with different legend designs and were asked to estimate the value at the location of a marker. The data was represented as a spatially distributed phenomenon mapped to opacity in a polygon that is overlaid on a base map, as in the ArcGIS online maps described in section \ref{background}.

Different designs are presented below, together with some justifications – where all legends and data mappings are linear in the opacity/alpha channel. This ought to introduce less complications than colour-mapping of colour due to a linear encoding of the alpha-channel in the RGBA-model:

>  *It’s worth pointing out that unlike the color components which are often encoded using a non-linear transformation, alpha is stored linearly – encoded value of 0.5 corresponds to alpha value of 0.5 (Ciechanowski, 2019)*

For more information on the process of example generations and technology choices see Appendix A.

**Baseline 1 - No legend ["Headline" in tables]**
```{r echo=FALSE}
figure_number = figure_number + 1 
```

Only having a title indicating range of values. Used to test correctness of visual decoding by subjects without legend. Presented as first and last example in the progression presented to the subjects.

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-headline-legend-merge.png){width=40%}


**Baseline 2 – ArcGIS Legend Imitation ["Checkered" in tables]**

```{r echo=FALSE}
figure_number = figure_number + 1 
```

The design is used as baseline for models that explore if alternative legend choices are helping in decoding data-values compared to the current "industry standard". 

This, and all following examples imitate the ArcGIS-design in the size of legend (constant for all legends). Legend located in the bottom right corner.

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-checkered-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```
 
**Legend with sampled context [Contextualizing] (Figure `r figure_number`) ["Sampled" in tables]** 

One likely downside making it more difficult for subjects to accurately decode the data values is the lack of context in the legend as it relates to the background of the data in the visualization. The rest of the example types works to provide designs that gives more of the actual map base-layer background information to the legend.

The simplest design choice was to sample part of the base layer as background as seen in Figure `r figure_number`. The details on how this was done in practice can be found in Appendix A.
 
![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-sampled-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

**Legend with clustered colour bands  (Figure `r figure_number`) ["Clustered" in tables]**

An alternative to sampling a box of the base layer map background, is to use the most common colours of the base layers as a background. In the design tested, the 10 most common colours of the background map was displayed as strips/columns in a vertical manner behind the data representation. This way they are contextualized for the full range of the data in the legend. More details how the common colours were identified are described in Appendix A. 

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-clustered-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

**Annotated Outline  (Figure `r figure_number`) ["Annotated" in tables]**

The legend placement may heavily affect the users ability to keep the information in near memory while moving the eyes back and fourth between visualized data and the legend.

In an attempt to move the information closer to the data – an outline legend placed next to the data-polygon was presented to the respondents. The legend is also contextualized by having the legend opacity superimposed over the base map layer in the same way as the actual data encoding.

In order to remove design choices of how to place this kind of legend around complex polygon shapes, a simple square-data area was chosen for all examples.



![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-annotated-legend-merge.png){width=40%}
 
<!-- ### *Data collection presentation and user interface* -->
\subsubsection{Data collection presentation and user interface} \label{data-collection-ui}


A front-end was developed using front-end web-technologies. In this system, the respondents were presented with a progression of stages:

1. Introduction with instructions, introducing the elements of the visualization and tasks to guess/estimate the value of the data layer at the location of a randomly generated marker position making use of the different legend types.

2. Randomly chosen images picked by the back-end server from the 315 example combination images (3 colours * 21 random data and marker location variations * 5 legend types = 315) stored in the back-end and presented to the respondents\footnote{
The progression of the images started and ended with the **Baseline 1 - No legend** type, and had the remaining 4 design type orders picked at random for the example progression images 2-5 and 6-9 respectively in order to maximize randomization. The colours of the data layer were for all examples available in pure red, green and blue. For the first image in the progression a combination of these three colours was randomly generated and repeated for examples 1-10. The reason for changing the colours between the examples was to reduce the risk of the respondents remembering the colour-to-data mapping between examples.

As an exmaple: for the progression of images 1-10, three lists are generated: a colour progression ["green", "blue", "red], a random list of numbers from the number of examples (21) [3, 14, 4, 10, 16, 2, 7, 9, 20], and selection of legend types [[first 5: no-legend{fixed}, random order of 4 remaining types], last 5: random order of 4 remaining types, [no-legend{fixed}]], then these were combined as ["3-green-no-legend", "14-sampled-blue", "4-red-clustered", "10-green-annotated", ..., "20-green-no-legend"]
}. Each response and a number of interactions by the respondents were sent back to the back-end server and persisted in a data base as the respondents went through the task. The response was picked using a number picker covering the range of the data (kept constant to 0-100). The initial value of the slider was random, and the respondents were required to change the response before being able to progress to the next example image.

3. After progressing through the example images, the respondents were asked about how helpful they considered the different legend types were for their task. This information was also sent back and persisted to the data base together with IDs of the respondents.

More information, example images of the user interface and text of the HTML markup can be found in Appendix B.

The survey data was collected between Jan 9th and Feb 6th 2021, using convenience sampling. Many of the 34 respondents are employees at the Volvo Car Corporation Asia Pacific Head Quarter in Shanghai, China.

The inital plan was to put the survey online using cloud-technologies for wider distribution. During the testing stage, it was however clear that there was a need to check that the instructions were understood to reduce the variation in the data. From this a more qualitative methodology was chosen where I was present in the room and asked the respondent if the task was understood after reading the instructions described in stage 1. of the list above. 

This resulted in a more consistent study, where the same laptop was used by all data collection, and the lighting conditions were possible to be kept consistent.


## *Data Analysis*

### *Data and variable description*


```{r import-data-and-clean, cache=TRUE, echo=FALSE}
source("./../data_read_in.R")
responses_an = df_from_db()


########################################
# Data Cleaning
########################################
error_uuids = c(
  "db5cf00b-448f-47cf-842c-8d237f6cf45b",
  "f0e25470-5bf5-4363-b75f-bc9f704dc9b1"
)


responses_an = responses_an %>% filter(!(uuid %in% error_uuids))

responses_an$uuid = as.factor(responses_an$uuid)
responses_an$maptype = as.factor(responses_an$maptype)
responses_an$colour = as.factor(responses_an$colour)
responses_an$legend = as.factor(responses_an$legend)

responses_an_viz = responses_an
responses_an_viz$legend <- factor(responses_an_viz$legend, levels = c("headline", "checkered", "sampled", "clustered", "annotated"))


```



**Treatment**

*LEGEND* - Each of the legend types, as the only part of visualisaion that is not randomized. Have values of no legend (headline), checkered, clustered, sampled and annotated corresponding to section \ref{legends}.

```{r response-error-table, echo=FALSE, message = FALSE}

tbl_summary(
  responses_an %>%
    select(legend)
) %>%
  modify_header(label = "*Total*") 

```

Each of the 34 respondent completed the full survey, resulting in 340 responses to the data-progression, and as seen they are distributed equally among the legend types.


**Dependent Variables**

*PERCEPTION ERROR* distance (delta) between the response value to the actual value of the location of the marker per visualization. 

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

```{r response-error-visualization, fig.pos = "h", fig.cap = "Perception error by visualization category", echo=FALSE}
ggplot(responses_an_viz, aes(legend, percept_error, color=legend)) + 
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  stat_summary(fun.data="mean_sdl", fun.args = list(mult = 1), 
               geom="pointrange", color="black") + 
  theme_minimal() +
  theme(legend.position="none") + 
  ylab("Perception Error")
```

As seen in Figure `r figure_number`, the mean is centered around 0, with some visually differences in distribution between the headline (no legend) category having the largest variance and more outliers. The figure has the mean indicated by a black dot, with 1 standard deviation in each direction shown by a range for each category.


*|PERCEPTION ERROR|* Absolute perception error - for linear modeling there is a need to look at mean-differences between categories, as simply using the error will result in all categorical differences being statistically insignificant.

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

As seen by Figure `r figure_number`, the same data visualized as boxplots with an absolute transformation of the perception error, shows the mean (visualized as a black point) differeing between the categories - highest for no legend, and lowest for annotated legend. As can also be seeen there are some outliers (visualized as dots outside above the "whiskers") for almost all categories \footnote {
defined for box plots as 1.5 times the interquartile  range [the height of the boxes] outside the interquartile range
} , except for headline/no legend, where the variance of the data is as quite large.


```{r abs-response-error-visualization, fig.cap = "Box plot - absolute error by visualization category", echo=FALSE}

ggplot(responses_an_viz, aes(legend, percept_error_abs, color=legend)) +
  geom_boxplot()  +
  stat_summary(fun="mean", geom="point", color="black") + 
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Absolute Perception Error")

```

*ACCEPTANCE* - objective valuation by participants on the usefulness of each legend type. On the scale 1-5, with responses of "No opinion" treated as missing values/Unknown. 

```{r acceptance-table, echo=FALSE, message = FALSE}
tbl_summary(
  responses_an_viz %>%
    select(uuid, acceptance, legend) %>%
    distinct() %>% select(-c(uuid)),
  by = legend
) %>%
  add_n() %>% # add column with total number of non-missing observations
  modify_header(label = "**Legend**") %>% # update the column header
  bold_labels()


```
Table **TODO-INSERT TABLE NUMBER** indicates that the highest *ACCEPTANCE* scores are for annotated and sampled legend types. More statistical comparison to follow in the results section. 
**TODO: WHY 164, if 33 would respond = 165, also for all 34, would be 170 + comment on this!!!**

**Independent/Control Variables Candidates**

*SUBMIT TIME* - time from the rendering of the example until the subject submitting their response. In study design expected to be a proxy for uncertainty.

*INPUT CHANGES* - Number of times the respondent changes their answer with input-slider before submitting. In study design expected to be a proxy for uncertainty.

*HOVER EVENTS* - Events of hovering the mouse cursor over the example image logged and sent back to back end. In study design expected to be a proxy for uncertainty.

Due to the more quantitative study design where the experiment designer was present in the room, it became clear that all these uncertainty-proxy variables would be likely to contain limited amount of signal. *SUBMIT TIME* varied because of incoming phone calls, the need to ask questions about the design types and many other external factors, *INPUT CHANGES* varied due to how the respondents chose to input their responses and work with the slider generally, and *HOVER EVENTS* did not vary much within-subjects, and wasn't consistently used as a visual guide.

Plots for these variables can be found in Appendix C.

*PROGRESSION* - For each respondents, numbering of the examples they were shown from 1-10. Later to be used to check for learning and/or fatigue effects.

*COLOUR* - Visualization presented having opacity data layer mapped colour channel as pure red, green or blue. From visual inspection, not any strong difference between the colours (see Appendix C). 


### *Hypothesis testing models*

Most of the research questions are centered around how different legend design choices affects errors and acceptance.

For *PERCEPTION ERROR* the average value is centered around 0, but the distribution variance is of interest. Most statistical models are constructed to compare in particular mean values between categories, which would not be able to give any indication of differences between these values. It would be possible to create a non-parametric test using bootstrapping of almost any test statistics (Efron, 1979). However, for simplicity Bartlett's test \footnote{
See https://en.wikipedia.org/wiki/Bartlett%27s_test and http://www.sthda.com/english/wiki/compare-multiple-sample-variances-in-r for comparisons of standard statistical test for variance differences
} allow testing of multiple variance differences between multiple groups. **TODO: REFERNECE**

The Bartlett test will be conducted for both the full data including the headling/no-legend type as baseline, and a subset to see if variances differs in a statistically significant way between the different legend types, using checkered legend type as a baseline, as the type having the largest variance test statistic of the sample.

Bartlett's test assumes normality, so the Shapiro-Wilk test **TODO: REFERENCE** are first to be conducted in order to see if the normality-assumption holds. This will be done for all legend types combined, with headline/no-legend removed, as well as for the subsample of each legend type separately.  

For *|PERCEPTION ERROR|*, *ACCEPTANCE* and other questions where a mean-comparison is able to be of relation to the reseach questions \footnote{
Including *SUBMIT TIME*, *INPUT CHANGES* and *HOVER EVENTS*, which will have the results presented in Appendix **TODO - APPENDIX LETTER**
} are examined using regular OLS-techniques (linear regression). Of importance for interpretable results is the baseline for these models (intercept), for ease of understanding p-values and test statistic differences. 

The baseline OLS-estimates are retained using 

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} LEGEND_{i} + \varepsilon_{i}
\end{equation*}

and

\begin{equation*}
ACCEPTANCE_{i} = \alpha + \beta_{1} LEGEND_{i} + \varepsilon_{i}
\end{equation*}

To account for individual fixed effects\footnote{Certain respondents are likely to have different average correctness due to within-subject variations}, this is in case of statistically significant findings also tested using individual fixed effects (FE)-models **TODO: REFERENCE**. This can be thought of as including separate intercept-variables for each respondent using dummy variables for each respondent\footnote{The statistical analysis software may choose to estimate this using other techniques, such as entity-demeaned OLS for computational efficiency https://www.econometrics-with-r.org/10-3-fixed-effects-regression.html}.

\begin{equation*}
|PERCEPTION ERROR|_{ij} = \alpha + \beta_{1} LEGEND_{i} + \gamma_{2} RESPONDENT2{j} + \gamma_{3} RESPONDENT3{j} + ... + \gamma_{n} RESPONDENTn{j} + \varepsilon_{ij}
\end{equation*}

The individual effects observed was not expected to be as significant as before the study was conducted, due to a more controlled environment for data collection as described in section \ref{data-collection-ui}.


### *Robustness checks*

"Time fixed effects", is considered to be added to the models to check for learning and/or fatigue effects as the respondent went through the progression of tasks. In order to see if this is of value to pursue further, first a simple OLS-model is tested to see if there are any indications of

Given that the subjects were given no feedback on the correctness of their responses during the progression, learning effects are likely to be negligible.

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION_{i} + \varepsilon_{i}
\end{equation*}

An alternative model will be run on the subset of only the first and last example subset (headline/no-legend), to see if these categories differ significantly.

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION LAST_{i} + \varepsilon_{i}
\end{equation*}

The results from these test will feed into the possible need to do subsample analysis using only e.g. the first 5 responses of each respondent.

Subset analysis using only the respondents that are likely to be the most engaged or skilled, as defined as the respondents with below median average *|PERCEPTION ERROR|* is to be conducted to see if the results differ in a significant way for those respondents.



# *Results*

## *PERCEPTION ERROR*

```{r baseline-data-processing, echo=FALSE, message = FALSE}
########################################
# ERROR AND DISPLAY TYPES
# 1. legends -> opacity mapping perception? baseline = no legend
# 2. alternative legend choices vs. standard? baseline = checkered
# 3. legend to data proximity-reduction|contextualised -> error? baseline = checkered
########################################

# Data sets
no_legend_baseline = responses_an
no_legend_baseline$legend = relevel(responses_an$legend, ref = "headline")
checkered_baseline = responses_an
checkered_baseline = checkered_baseline %>% filter(legend != "headline")
checkered_baseline$legend = relevel(checkered_baseline$legend, ref = "checkered")

```


### *Normality tests*

```{r normality-testing, echo=FALSE, message = FALSE}


test_normality <- function(df, legend_type){
  if(legend_type=="combined"){
    perception_error_only_df = responses_an %>% select(percept_error)
  } else if(legend_type=="headline_removed"){
    perception_error_only_df = responses_an %>% filter(legend!="headline") %>% select(percept_error)
  }
  else{
    perception_error_only_df = responses_an %>% filter(legend==legend_type) %>% select(percept_error)
  }
  normality = shapiro.test(perception_error_only_df$percept_error)
  normality_results = ifelse(normality$p.value > 0.05, paste(legend_type, "NORMALity not rejected"), paste(legend_type, "NON-NORMAL"))
  # print(normality_results)
  return(normality)
}

# Sensitive to normality
legend_types = c(
  "combined",
  "headline_removed",
  "headline",
  "sampled",
  "clustered",
  "checkered",
  "annotated"
)


normality_list <- data.frame()

for(legend in legend_types){
  normality_results <- test_normality(responses_an, legend)
  
  normality_list <- rbind(normality_list, 
                           data.frame(legend=legend, 
                             test_statistics = normality_results$statistic[[1]],
                             p_value = normality_results$p.value[[1]])
  )
}

normality_list$legend <- c("Combined", "Headline Removed", "Headline", "Sampled", "Clustered", "Checkered", "Annotated")

names(normality_list) <- c("Sample", "Shapiro Wilk Test Statistics", "P-Value")

```

As shown in the table below **TODO: TABLE NUMBERING**, all the p-values for the Shapiro-Wilk test is above 0.05, meaning the the null of normality is not rejected for any of the tests run. In otheer words, we can assume that the data is normally distributed. 

```{r normality-testing-table, echo=FALSE, message = FALSE, results='asis'}

library(stargazer)
stargazer(normality_list,type='latex', summary = FALSE, header = FALSE)


```


### *Bartlett test of homogeneity of variances*


```{r processing-variance-ordering, echo=FALSE, message = FALSE}

# check which category has the highest variance
varince_ordering <- no_legend_baseline %>%
  group_by(legend) %>%
  summarize(sd = sd(percept_error)) %>%
  arrange(desc(sd))


varince_ordering$legend <- plyr::revalue(varince_ordering$legend, c(
  headline="Headline", 
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))



names(varince_ordering) <- c("Legend", "Standard Dev.")

varince_ordering$Legend <- as.character(varince_ordering$Legend)
varince_ordering$`Standard Dev.` <- round(varince_ordering$`Standard Dev.`, 1)
```

```{r processing-variance-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(varince_ordering,type='latex', summary = FALSE, header = FALSE)
```

Table **TODO: TABLE NUMBERING** shows that the example with no-legend/headling shows largest variance/standard deviation, followed by checkered, sampled, clustered and annotated legend types.

In the statistical tests the headline type is used first as baseline, to see if these differences are statistically significant, then a subset removing all the headline data points from the sample is used, and see if the variance differ significantly.

```{r bartlett-variance-models, echo=FALSE, message = FALSE}

bartlett_model_all = bartlett.test(percept_error ~ legend, data = no_legend_baseline)
# ifelse(
#   bartlett_model_all$p.value < 0.05,
#   "Variance SIGNIFICANT difference",
#   "Variance NO significant difference"
# )


bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_df <- data.frame(
  "Sample" = c(
    "All, including Headline",
    "Excluding Headline"
  ),
  "Test Statistic" = c(
    bartlett_model_all$statistic,
    bartlett_model_against_checkered$statistic
    ),
  "P Value" = c(
    bartlett_model_all$p.value,
    bartlett_model_against_checkered$p.value
  )
)

```
```{r bartlett-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(bartlett_df,type='latex', summary = FALSE, header = FALSE)
```

As seen from Table **TODO: TABLE NUMBERING**, there is a statistically significant difference in variance for the sample including the Headline type (p-val < 0.05), but that significance does not remain when excluding the headline/no-legend category. This indicates that there is not much need to look into two-way comparisons between the legend types.

For estimations of size of differences between groups transformation of *PERCEPTION ERROR* to absolute terms is analyzed instead.

## *|PERCEPTION ERROR|*

Visual inspection of whether there are clear difference in heterogenity in *|PERCEPTION ERROR|* for both legend types and between respondents are seen in **TODO: FIGURE NUMBERING** in Appendix C (bars including 95% confidence intervals). The confidence intervals are largely overlapping for all types except when not including any legend (Headline), which is in line with results from variance testing. For respondent variation, there are large variations, but still a large fraction that have overlapping confidence bounds.

```{r abs-perception-error-models, echo=FALSE, message = FALSE}
library(plm)
library(lmtest)
library(zoo)

# Absolute error - simple OLS
## 1. legend useful at all
### Baseline
baseline_legend_of_use <- lm(percept_error_abs ~ legend,
                     data = no_legend_baseline)

### FE = robustness
fe_legend_of_use <- plm(percept_error_abs ~ legend,
                          data = no_legend_baseline,
                          index = c("uuid"),
                          model = "within")
# print summary using robust standard errors
# coeftest(fe_legend_of_use, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_legend_of_use)


## 2. Legend vs standard = checkered
## 3. annotated or contextualized also answered
### Baseline
standard_baseline_difference <- lm(percept_error_abs ~ legend,
                             data = checkered_baseline)

### FE = robustness
fe_standard_difference <- plm(percept_error_abs ~ legend,
                            data = checkered_baseline,
                            index = c("uuid"),
                            model = "within")
# print summary using robust standard errors
# coeftest(fe_standard_difference, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_standard_difference)


```

```{r abs-perception-error-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  baseline_legend_of_use,
  # fe_legend_of_use,
  # standard_baseline_difference,
  # fe_standard_difference,
  # title="Regression Results"#, 
  # align=TRUE, 
  # header = FALSE,
  type='latex'
  )

```









```{r name-of-chunk2, echo=FALSE, message = FALSE}

# ########################################
# # ACCEPTANCE
# # legend choices subjective -> how do users percieve these types? Useful/not useful
# ########################################
# 
# # NEED TO USE DATA WITHOUT DUPLICATES
# 
# acceptance_data_unique = responses_an %>% 
#     select(uuid, acceptance, legend) %>% 
#     distinct()
# 
# # Generally by group, which highest
# acceptance_data_unique %>% 
#   group_by(legend) %>% 
#   summarize(mean = mean(acceptance, na.rm = T)) %>%
#   arrange(desc(mean))
# # 1 sampled    4.18
# # 2 annotated  4.06
# # 3 checkered  3.47
# # 4 clustered  3.09
# # 5 headline   2.09
# 
# # Checkered over clustered... let's keep still
# 
# # Earlier baselines still makes rough sense
# acceptance_data_unique_legend_baseline = acceptance_data_unique
# acceptance_data_unique_legend_baseline$legend = relevel(acceptance_data_unique_legend_baseline$legend, ref = "headline")
# acceptance_data_unique_checkered_baseline = acceptance_data_unique
# acceptance_data_unique_checkered_baseline = acceptance_data_unique_checkered_baseline %>% filter(legend != "headline")
# acceptance_data_unique_checkered_baseline$legend = relevel(acceptance_data_unique_checkered_baseline$legend, ref = "checkered")
# 
# 
# # Models- all significant against no-legend (= preferred over)
# summary(lm(acceptance ~ legend, 
#            data = acceptance_data_unique_legend_baseline))
# # Annotated and sampled significantly preferred, clustered lower, but not significantly so
# summary(lm(acceptance ~ legend, 
#            data = acceptance_data_unique_checkered_baseline))


```

```{r name-of-chunk3, echo=FALSE, message = FALSE}

# ########################################
# # DECISION TYPES
# # legend choices -> user selection behaviour?
# ########################################
# 
# # As shown earlier - a lot of variance, not a lot of signal
# # Outliers likely affecting, simple OLS only
# 
# # Time until submit -> likely because of first interaction as legend
# time_submit_all <- lm(submitTime / 1000 ~ legend, 
#                              data = no_legend_baseline)
# summary(time_submit_all)
# # Between those with legend, no statistically significant difference difference compared to baseline
# time_submit_w_legend <- lm(submitTime / 1000 ~ legend, 
#                   data = checkered_baseline)
# summary(time_submit_w_legend)
# 
# # inputChanges - only on first interaction, nothing significant vs. checkered
# summary(lm(inputChanges ~ legend, 
#            data = no_legend_baseline))
# summary(lm(inputChanges ~ legend, 
#            data = checkered_baseline))
# 
# # hoverEvents - no significant difference, high std errors, order of baseline category will not matter
# summary(lm(hoverEvents ~ legend, 
#            data = no_legend_baseline))
# summary(lm(hoverEvents ~ legend, 
#            data = checkered_baseline))

```


## Robustness tests


```{r name-of-chunk4, echo=FALSE, message = FALSE}

# ########################################
# # SUBSET ANALYSIS - ROBUSTNESS
# ########################################
# 
# 
# ########
# # Correlation error with progression 
# # learning effects as progression or getting tired of task... (INDIVIDUAL+PROGNUM)
# progression_effect_mod <- lm(percept_error_abs ~ prog, 
#                              data = responses_an)
# summary(progression_effect_mod)
# # Nothing significant, not going to take this into account - or do analysis using e.g. first 5 images only
# 
# # TODO: Baseline 1 vs baseline 10 - difference? - SUBSAMPLE ONLY 1st and 10th!!!
# 
# ########
# # Respondents with smaller average errors (outlier removal) - top 50th percentile
# 
# num_respondents_chosen = (responses_an$uuid %>% unique() %>% length())/2
# 
# arranged_respondents_by_error = responses_an %>% group_by(uuid) %>%
#   summarize(mean_abs_error = mean(percept_error_abs)) %>%
#   arrange(mean_abs_error) 
# 
# resp_low_errors = arranged_respondents_by_error[1:num_respondents_chosen,] %>% select(uuid)
# 
# resp_low_errors_df = responses_an %>% filter(uuid %in% resp_low_errors$uuid)
# 
# # Absolute
# # Not really anything that sticks out
# ggplot(resp_low_errors_df, aes(legend, percept_error_abs, color=legend)) + 
#   geom_boxplot()  +
#   stat_summary(fun="mean", geom="point", color="black")
# 
# # For completeness
# no_legend_baseline_accurate = resp_low_errors_df
# no_legend_baseline_accurate$legend = relevel(no_legend_baseline_accurate$legend, ref = "headline")
# checkered_baseline_accurate = no_legend_baseline_accurate
# checkered_baseline_accurate = checkered_baseline_accurate %>% filter(legend != "headline")
# checkered_baseline_accurate$legend = relevel(checkered_baseline_accurate$legend, ref = "checkered")
# 
# # Absolute error - simple OLS
# ## 1. legend useful at all
# ### Baseline
# baseline_legend_of_use_accurate <- lm(percept_error_abs ~ legend, 
#                              data = no_legend_baseline_accurate)
# summary(baseline_legend_of_use_accurate)
# ### FE = robustness
# fe_legend_of_use_accurate <- plm(percept_error_abs ~ legend,
#                         data = no_legend_baseline_accurate,
#                         index = c("uuid"),
#                         model = "within")
# # print summary using robust standard errors
# coeftest(fe_legend_of_use_accurate, vcov. = vcovHC, type = "HC1")
# # Nonrobust -> summary(fe_legend_of_use)
# 
# 
# ## 2. Legend vs standard = checkered
# ## 3. annotated or contextualized also answered
# ### Baseline
# standard_baseline_difference_accurate <- lm(percept_error_abs ~ legend, 
#                                    data = checkered_baseline_accurate)
# summary(standard_baseline_difference_accurate)
# ### FE = robustness
# fe_standard_difference_accurate <- plm(percept_error_abs ~ legend,
#                               data = checkered_baseline_accurate,
#                               index = c("uuid"),
#                               model = "within")
# # print summary using robust standard errors
# coeftest(fe_standard_difference_accurate, vcov. = vcovHC, type = "HC1")
# # Nonrobust -> summary(fe_standard_difference)




```



# Discussion

**Here you discuss the results by putting your findings in relation to what others published on the matter. Present a critical analysis of your own results, and discuss the sources of error.**


Limitations and some mitigations
•	The study is focusing on static and non-interactive maps - no tooltips or animations to help the user.
•	Only one non-opaque layer will be used, eliminating issues such as combined opacity of overlapping layers. The opacity-mapped layer is assumed to be the only data mapping of interest apart from the geographical context.
•	Non-random sampling – for possibility of larger response rate – being able to point people to a URL to be surveyed is distinctively non-random. Trying to rely largely on a within-subject methodology could mitigate somewhat, and I don’t see a specific skew such as having asked e.g. only students at a geography department as being likely.
•	Learning effects are likely if the subject is asked to repeat similar tasks multiple tasks. This will be mitigated through randomizing the progression, while likely keeping 1 baseline example at start and 1 at end.


List feedback:
- Larger legends, but imitated ArcGIS as closely as possible
- Why not relative error
- Why not subset of people using interactions: not indicative. Some use arrows to make changes 0.1 at a time, interactions topped at max number… Some hover while talking generally about the problem, some don’t hover when they do careful comparisons using hands on screen


FOR REFERENCE LIST: 
Specifications for canvas
Hierarchical clustering of image
Perception of color 
Other literature on Perception with clutter/backgrounds/overlays
Greenwich as center
Cluster as concept needs to be described

Delta: 
Trying to isolate to ONLY difference being legend display
Legend design (not large, only min max actually showed) -> same as ArcGIS as baseline, need to visually interpolate or try to interact with image to get feeling for values in between
Detailed description of dev choices -> prog start/end with baseline, rest all randomised
If all visualizations same range, may stop making use of legends after getting used to mapping -> ISSUE BUT OTHERWISE NEED TO !NORMALIZE! ERROR TO % OF RANGE SELECTION? 
Use of colours to not be able to remember mapping as easy...


<!-- TODO: Migrate references!!! -->
<!-- # References

Adelson, Edward (1993). ‘Perceptual Organization and the Judgment of Brightness’. Science, 262, p2042-2044
ArcGIS (2020a) Create a custom visualization using Arcade [Online]. Available at: https://developers.arcgis.com/javascript/latest/sample-code/visualization-arcade/index.html (Accessed: 13 Sept 2020)
Berry, Lisa (2016) 6 Easy Ways to Improve Your Maps [Online]. Available at: https://www.esri.com/arcgis-blog/products/mapping/mapping/6-easy-ways-to-improve-your-maps/#:~:text=%206%20Easy%20Ways%20to%20Improve%20Your%20Maps,sometimes%20become%20distracting%20and%20tear%20your...%20More%20 (Accessed: 13 Sept 2020)
Brehmer, M. Munzner, T (2013) ‘A Multi-Level Typology of Abstract Visualization Tasks’. IEEE Transactions on Visualization and Computer Graphics, 19(12), p 2376 - 2385
Brewer, Cynthia (2006). ’ Basic Mapping Principles for Visualizing Cancer Data Using Geographic Information Systems (GIS)’. American Journal of Preventive Medicine, 30(2S).
Cairo, Alberto (2016) The Truthful Art:Data, Charts, and Maps for Communication. Berkeley, CA: New Riders.
Ciechanowski, Bartosz (2019). Alpha Compositing [Online]. Available at: https://ciechanow.ski/alpha-compositing/ (Accessed: 20 Feb 2021)  
Chan, MY, Wu, YC, Mak, WH, Chen, W (2009). ‘Perception-Based Transparency Optimization for Direct Volume Rendering’. IEEE Transactions on Visualization and Computer Graphics, 15(6).
Cleveland, WS, McGill, R (1985) ‘Graphical Perception and Graphical Methods for Analyzing Scientific Data’. Science, 229(4716), p828-833.
Correa, Carlos D, Ma Kwan-Liu (2008). ‘Size-based Transfer Functions: A New Volume Exploration Technique’.  IEEE Transactions on Visualization and Computer Graphics, 14(6).

Dykes, J., Wood, J. and Slingsby, A. (2010). ’Rethinking Map Legends with Visualization’. IEEE Transactions on Visualization and Computer Graphics, 16(6), p.890-899.

Ee, Shaun (2020). Back to school with Tencent and friends - Health surveillance goes into schools as some classes resume [Online]. Available at: https://technode.com/2020/04/28/health-code-back-to-school-with-tencent-and-friends/ (Accessed: 13 Sept 2020)

Efron, Bradley. (1979). Bootstrap methods: Another look at the jackknife. The Annals of Statistics. 7 (1): 1–26. doi:10.1214/aos/1176344552.

Grant, Robert (2019) Data Visualization – Charts, Maps and Interactive Graphics. Boca Raton, FL: CRC Press.
Jaemin Jo, Frédéric Vernier, Pierre Dragicevic, Jean-Daniel Fekete. ’A Declarative Rendering Model for Multiclass Density Maps’. IEEE Transactions on Visualization and Computer Graphics, Institute of Electrical and Electronics Engineers, 2019, 25 (1), pp.470-480.
Kim, YS, Walls ,LA, Krafft , P, Hullman, J (2017a). A Bayesian Cognition Approach to Improve Data Visualization. ACM Conference (Conference’17). ACM, New York, NY, USA.
Kim, YS, Reinecke, K, Hullman, J (2017b). Explaining the Gap: Visualizing One’s Predictions Improves Recall and Comprehension of Data. 2017 CHI Conference on Human Factors in Computing SystemsMay 2017 Pages 1375–1386
Metelli, Fabio (1974) ‘The Perception of Transparency’. Scientific American, 230(4), p90-99.
Midtbø, Terje (2007). ‘Advanced Legends for Interactive Dynamic Maps’. 23rd International Cartographic Conference, Moscow
Munzner, Tamara (2015) Visualization Analysis and Design. Boca Raton, FL: CRC Press.
Perlin, Ken (1985) ‘An Image Synthesizer´. Computer Graphics, 19(3), p287-296
Roth, RE, Woodruff AW, Johnson, ZF (2010). ’Value-by-alpha maps: An alternative technique to the cartogram’. The Cartographic Journal, 47(2) p130–140.
Tufte, Edward (2001). The Visual Display of Quantitative Information. 2nd edition. Cheshire, CO: Graphics Press.
Ware, Colin (2020) Information Visualization – Perception for Design. 4th edition. Camebridge, MA: Elsevier.

-->

<!-- # Appendices -->
\newpage

\section{Appendices} \label{appendices}


## Appendix A - example generations and technology choices

*TECH CHOICES*
*- Example Generation*
  *- Dev environment*
  *- Canvas*
  *- Perlin noise, The spatially generated data can be generated using a (2D) Perlin noise function (base version introduced in Perlin, 1985).*
  *- Randomization colour*
  *- Number of examples*
  *- All combinations using Node, exposed as png*
  
  - Design choices
    - Area of baselayer
- Elements
- Technology choices
- For ease to vary the legend choices, generated within an area, (within a range to leave space for legends) and simple data layer in a square
- How to generate sampled background 
- How to generate clustered background
- Range 0-100

Map Base Layer
Base layer using open street map, without license use restrictions – geographic coverage and zoom-level to be decided during the prototype construction phase. The underlying mapped area should also considered – one solution could simply be to choose an area for the base layer that is unlikely to be known to respondents – likely needed discussion prior to study.


Colour choice
Colour is a consideration – but for simplicity, simply choosing a full RGB-channel colours – i.e. either completely red, blue or green, and simply vary only alpha/opacity channel. Which of the three colours to use one could be one of the factors considered after first prototype have been developed.

Prototype Graphics Library Selection
Modern web browsers support three graphic libraries: SVG, Canvas and WebGL. A more thorough discussion on the likely use of the Canvas library for prototype is discussed in “Appendix A – Web Graphics Library Discussion”.

Manual blending from background image is likely to be very computationally heavy – making this probably more suitable to be done offline, and generate a number of exported image files that can be sent over to the user, instead of dynamically generated and sent back to the server. ArcGIS online have also used Canvas to display the images to the user – and the calculated image data is likely done server side.




\newpage

## Appendix B - User interface and data serving setup

*- UI + Backend*
*- Example - intro-page, screeenshots full interface, acceptance content and similar...*
*- Progression*


UI - introduction page
Pictures of progression
HTML markup
Backend
UUID generation


1. Session-level logging
•	Respondent-ID, UUID4 generated by the server
•	Self-reported impression after conducted examples – which presentation types were intuitive and where the legend designs considered useful 
2. Event-logging
•	Timestamp logging at time of image load
•	Any interactions with input mechanisms with timestamp – e.g. any change of slider values (not just final answer)
•	Time of submitting answer
•	Interactions with visualization (mouse movements over image with coordinates and timestamp, as eye movement can not be measured).



\newpage

## Appendix C - Plots and Tables

```{r submit-time-visualization, fig.cap = "Submit time", echo=FALSE}

# Submit time
ggplot(responses_an_viz, aes(legend, submitTime, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Time to Submit")

```

```{r input-changes-visualization, fig.cap = "Input changes", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, inputChanges, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Number of Input Changes before Submit")

```


```{r hover-events-visualization, fig.cap = "Hover events", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, hoverEvents, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Number of Hover Events before Submit")

```


```{r colour-visualization, fig.cap = "Hover events", echo=FALSE}


ggplot(responses_an_viz, aes(colour, percept_error_abs, color=colour)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.5) +
  scale_color_manual(values=c("blue", "green", "red")) +
  stat_summary(fun="mean", geom="point", color="black") +
  ylab("Absolute Perception Error")+
  xlab("Colour") +
  theme_minimal() +
  theme(legend.position="none")
  

```




```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(gplots)
plotmeans(percept_error_abs ~ legend, main="Heterogeineity across legend types", data=no_legend_baseline)
plotmeans(percept_error_abs ~ uuid, main="Heterogeineity across respondents", data=no_legend_baseline, connect = F, n.label = F)


# PLOTS
# TABLE
# https://thatdatatho.com/2018/08/20/easily-create-descriptive-summary-statistic-tables-r-studio/
```








