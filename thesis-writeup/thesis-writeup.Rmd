---
output: 
  pdf_document:
    citation_package: natbib
    # keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    number_sections: true  
header-includes:
  -  \usepackage{hyperref}
title: "Opacity data perception support using legends when overlaid on complex map backgrounds"
---

<!-- TODO : MATCH WITH EXPECTATIONS FROM FORMATTING AND CONTENT OF START PAGE -->
<!-- TODO : **ABSTRACT** -->



\newpage
\tableofcontents 
\newpage


```{r, setup, include=FALSE}
# knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
library(mongolite)
library(stringr)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(gtsummary)
```

```{r echo=FALSE}
figure_number = 0 
```



\section{Introduction} \label{introduction}

## Data Visualization and GIS

There are clear overlapping goals between the area of data visualization - with the target visually encoding data attributes in a way that is visually pleasing, and easy for the target audience to decode - and visual representation of data on a map in a GIS-system. It may also be possible to consider visual representation of data for an end user in a GIS-system as a subfield of data visualization, and related fields such as the study of human perception, where most textbooks in these fields contains a section on geographic mapping of data (see e.g. Munzner, 2015, p. 181; Grant, 2019; Cairo 2016; Ware, 2020, p. 105, 125, 128ff).


## Opacity/transparency in GIS visualizations

Colin Ware (2020) highlights this relation between the fields of Data Visualization and GIS in his seminal work on information visualization in relation to the use of transparency as a technique to the use of transparency (alpha channel mapping/encoding) in information visualization:

> *In many visualization problems, it is desirable to present data in a layered form. This is especially common in geographic information systems (GISs). So that the contents of different layers are simultaneously visible, a useful technique is to present one layer of data transparently over another; however, there are many perceptual pitfalls in doing this. The contents of the different layers will always interfere with each other to some extent, and sometimes the two layers will fuse perceptually so that it is impossible to determine to which layer a given object belongs (ibid. p. 217).*

```{r echo=FALSE}
figure_number = figure_number + 1 
```


Ware (ibid, p. 357) also includes a figure which shows a fully opaque data encoding on top of a map layer (Figure `r figure_number`) – which depending on need can make it difficult for the end user to know the demarcations of the different categories due to occlusion.

 ![Figure `r figure_number`. Fully opaque data encoding resulting in lower layers not being visible, possible resulting in the end user having difficulties knowing the exact underlying geography.](./img/picture1.png)


```{r echo=FALSE}
figure_number = figure_number + 1 
```


The increased accessibility and ease to create maps for online usage makes different data mappings available, there are a lot of maps that make liberal use of a mix of mapping techniques, such as colour and opacity (Figure `r figure_number`) without taking into account the end-users ability to easily decode the data being visualized.
 
 ![Figure `r figure_number`. Liberal web-GIS use of multiple encodings without clear data mapping clarifications for end-user (Ee, 2020).](./img/picture2.png)


```{r echo=FALSE}
figure_number = figure_number + 1 
opacity_picker = figure_number
```


The availability and ease of using opacity mappings and other techniques are also shown by the market leader
\footnote{
”Esri builds the leading mapping and spatial analytics software for desktop, software as a service (SaaS), and enterprise applications. Esri ArcGIS products are designed to deliver location intelligence and meet digital transformation needs for organizations of all sizes. Here you can explore Esri software, apps, content, solutions, and developer tools.” - https://www.esri.com/en-us/arcgis/products/index Accessed 13 Sept 2020

”[…]our market-leading Enterprise GIS mapping software will support your work and deliver results” - https://www.esri.com/en-us/arcgis/products/arcgis-enterprise/overview Accessed 13 Sept 2020

} 
ESRI/ArcGIS examples and tutorials for their products, such as “ArcGIS online” and “ArcGIS Enterprise Map Viewer” are shown by the blog-entry “6 Easy Ways to Improve Your Maps”, where “4) Utilize Transparency” is included. The ways the transparency tool works is included as well (Figure `r opacity_picker`) does not give the user much information or consider underlying layer colors and effects on perception.

 ![Figure `r opacity_picker`. Transparency data mapping/encoding for ArcGIS products (Berry, 2016)](./img/picture3.png)

```{r echo=FALSE}
figure_number = figure_number + 1 
dark_background_example = figure_number
figure_number = figure_number + 1 
bright_background_example = figure_number
```


This mapping has a similar visual representation to the end user of the maps as shown in the usage examples from the “ArcGIS Online” API documentation ArcGIS website (Figure `r dark_background_example` and `r bright_background_example`). Figure `r dark_background_example` includes colour mapping, but the legend maps transparency into the same checkered grey/white background as for the tool shown in Figure `r opacity_picker`, making it very difficult to decode the visual values. Figure `r bright_background_example` has a single colour and a background layer corresponding roughly to the constant mid-grey and white checkered pattern.  

 ![Figure `r dark_background_example`. Opacity data mapping and legend with colour scale (ArcGIS, 2020a)](./img/picture4.png)
 
 ![Figure `r bright_background_example`. Opacity data mapping and legend (ArcGIS, 2020b)](./img/picture5.png)

This type of legend representation of opacity is in not context-aware, and remains a checkered mid-grey background no matter the background of the map. 

In static 2D maps, without interactive tools, such as tooltips – or in areas where hovering effects can not be as easily used (such as when using a tablet or smart phone), the end user have to rely on attempts to decode the visual representations with basic tools such as legends.

As will be discussed in section \ref{background}, legends have been studied, but not with focus on transparency decoding or attempts to use alternative techniques for transparency legends, such as contextualizing the legend or trying to increase the proximity of the to the encoded data. 

## Research Questions

- How is data opacity mapping perception influenced by legend display choices?
- Does alternative legend methods compared to GIS industry standard’s choice influence errors in perceived data mapping and colour choices?
- Does legend to data proximity-reduction or contextualised legend design influence perception errors for opacity data mapping?
- Can decision times and user selection behaviour be affected by legend choices?
- How are alternative legend design choices received by the subjects (subjective user acceptance testing)?



<!-- # Background -->
\section{Background} \label{background}

*TODO: INCLUDE MORE MEAT IN LITERATURE SECTION!!!*
**The background put the study in a wider perspective and gives a summary or ‘state of the art’ of the actual question. This chapter can present the literature study as a base to the problem and posed question. Already here you can present actual literature/references to research in the field of question, but this should be a presentation. Often you can use this summary of the stat of the art again the discussion of your results. Take care that you have the latest publications in the field of research topic included.**

## Previous related literature

<!-- The literature review will touch on several areas which intersect this study scope:  -->
<!-- i) data visualization and perception, low-level and design studies ii) transparency-focused studies iii) cartography and GIS visualization, including colour studies iv) use of legends. -->
<!-- These areas have certain number of overlap, but this grouping progressively moves towards the focus of this study. -->

<!-- i) Data Visualization and Perception -->
<!-- Brehmer and Munzner (2013) describes a gap between low-level tasks, and high-level tasks in the data visualization literature.  -->
<!-- Representative for the low-level perception-focused literature is the early studies focusing on measuring the ability of humans to correctly perceive different visual data encodings, such as position, length, angle, circle area, hue, etc. (Cleveland and McGill, 1985). -->
<!-- Other examples of low-level perception studies are Adelson’s (1982 and 1993) studies on perception effects from context, such as luminosity perception of a grey area depending on surrounding areas. -->
<!-- For the higher-level tasks, Tufte (2001) exemplifies attempts to bring these smaller encodings together to more stringently describe good data visualization practices. -->
<!-- Newer literature in the area commonly attempts to bridge the gap towards other scientific areas (Kim et al. 2017a, Kim et al. 2017b), incorporating mental models, uncertainty and Bayesian prior beliefs of subjects into the perception task. -->


<!-- ii) Transparency -->
<!-- Transparancy models have been used and studied extensively, with early opacity/transparency models by Fabio Metelli (1974) still being in use to this day. -->
<!-- TODO: rewrite, currently citing Ekroll's Transparency perception: the key to understanding simultaneous color contrast -->
<!-- Mitelli’s well-established additive model of perceptual transparency, a transparent layer of color L in front of a background region of color B produces a proximal color signal P which is simply a weighted mixture -->

<!-- <!-- P = \alpha·B+(1−\alpha)*L --> -->

<!-- Current transparency literature is heavily focused on computational rendering in a 3D/volume rendering context (e.g. Chan et al. 2009 and Correra & Ma, 2008). -->


<!-- iii) Cartography and GIS -->
<!-- Colour choices in a cartographic context is a large literature, with Brewer (2006) having provided colour palette tools commonly used throughout data visualization from studying colour use from a cartographic/GIS perspective. -->
<!-- Studies that cover transparency directly or indirectly are more rare. Jo et al. (2019) briefly covers opacity: "To scale scatterplots, several approaches have been proposed, such as adaptive opacity [15, 30, 32] and aggregation [13, 53]. However, adaptive opacity does not scale well with the number of categories since multiple categorical colors become ambiguous when blended[..]". -->
<!-- Another use of transparency with geospatial data in a design study setting is the 'value-by-alpha-map' described as: "A value-by-alpha map relates the alpha channel of each displayed enumeration unit to the unit's value in an equalizing variable. The value-by-alpha map solves the dilemma presented by Cartogram3. Shape and topology are both preserved perfectly because geography is maintained." (Roth et al. 2010). "Cartogram3" included as “Figure 3-1” below. -->

<!-- Figure 3-1. Value-by-alpha map comparison (Roth et al. 2010) -->


<!-- iv) Legends -->
<!-- Literature on use of legends seem highly tied to different visualization types. Recent map legend literature found have focused on interactive aspects (Midtbø, 2007) and attempts to provide guidance on the use of map legends in a “dynamic environment” (Dykes et al. 2010). -->
<!-- No intersection between legends and transparency either in GIS-context or elsewhere have been found in the literature review. -->

<!-- **Could flesh this out more with description of opacity legends in academic journals (sometimes included, not focus of studies in questions).** -->


# *Methodology*

**Here you present the applied methods for the collection of data, the analysis*, INCLUDE MOTIVATIONS**

## *Data Collection*


\subsubsection{Legend design types} \label{legends}
<!-- ### *Legend design types* -->

Due to the narrowness of the reasearch questions, no openly available dataset or examples was likely to exist. This resulted in extensive work to design and create a dataset and a reproducable experiment where legend types was possible to isolate as the only non-randomly changing treatment.  

The data was limited to static 2D maps and using web technologies for easy distribution. The subjects were presented with different legend designs and were asked to estimate the value at the location of a marker. The data was represented as a spatially distributed phenomenon mapped to opacity in a polygon that is overlaid on a base map, as in the ArcGIS online maps described in section \ref{background}.

Different designs are presented below, together with some justifications – where all legends and data mappings are linear in the opacity/alpha channel. This ought to introduce less complications than colour-mapping of colour due to a linear encoding of the alpha-channel in the RGBA-model:

>  *It’s worth pointing out that unlike the color components which are often encoded using a non-linear transformation, alpha is stored linearly – encoded value of 0.5 corresponds to alpha value of 0.5 (Ciechanowski, 2019)*

For more information on the process of example generations and technology choices see Appendix A.

**Baseline 1 - No legend ["Headline" in tables]**
```{r echo=FALSE}
figure_number = figure_number + 1 
```

Only having a title indicating range of values. Used to test correctness of visual decoding by subjects without legend. Presented as first and last example in the progression presented to the subjects.

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-headline-legend-merge.png){width=40%}


**Baseline 2 – ArcGIS Legend Imitation ["Checkered" in tables]**

```{r echo=FALSE}
figure_number = figure_number + 1 
```

The design is used as baseline for models that explore if alternative legend choices are helping in decoding data-values compared to the current "industry standard". 

This, and all following examples imitate the ArcGIS-design in the size of legend (constant for all legends). Legend located in the bottom right corner.

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-checkered-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```
 
**Legend with sampled context [Contextualizing] (Figure `r figure_number`) ["Sampled" in tables]** 

One likely downside making it more difficult for subjects to accurately decode the data values is the lack of context in the legend as it relates to the background of the data in the visualization. The rest of the example types works to provide designs that gives more of the actual map base-layer background information to the legend.

The simplest design choice was to sample part of the base layer as background as seen in Figure `r figure_number`. The details on how this was done in practice can be found in Appendix A.
 
![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-sampled-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

**Legend with clustered colour bands  (Figure `r figure_number`) ["Clustered" in tables]**

An alternative to sampling a box of the base layer map background, is to use the most common colours of the base layers as a background. In the design tested, the 10 most common colours of the background map was displayed as strips/columns in a vertical manner behind the data representation. This way they are contextualized for the full range of the data in the legend. More details how the common colours were identified are described in Appendix A. 

![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-clustered-legend-merge.png){width=40%}

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

**Annotated Outline  (Figure `r figure_number`) ["Annotated" in tables]**

The legend placement may heavily affect the users ability to keep the information in near memory while moving the eyes back and fourth between visualized data and the legend.

In an attempt to move the information closer to the data – an outline legend placed next to the data-polygon was presented to the respondents. The legend is also contextualized by having the legend opacity superimposed over the base map layer in the same way as the actual data encoding.

In order to remove design choices of how to place this kind of legend around complex polygon shapes, a simple square-data area was chosen for all examples.



![Figure `r figure_number`. Baseline - Opacity data mapping layer without legend](./img/020-red-annotated-legend-merge.png){width=40%}
 
<!-- ### *Data collection presentation and user interface* -->
\subsubsection{Data collection presentation and user interface} \label{data-collection-ui}


A front-end was developed using front-end web-technologies. In this system, the respondents were presented with a progression of stages:

1. Introduction with instructions, introducing the elements of the visualization and tasks to guess/estimate the value of the data layer at the location of a randomly generated marker position making use of the different legend types.

2. Randomly chosen images picked by the back-end server from the 315 example combination images (3 colours * 21 random data and marker location variations * 5 legend types = 315) stored in the back-end and presented to the respondents\footnote{
The progression of the images started and ended with the **Baseline 1 - No legend** type, and had the remaining 4 design type orders picked at random for the example progression images 2-5 and 6-9 respectively in order to maximize randomization. The colours of the data layer were for all examples available in pure red, green and blue. For the first image in the progression a combination of these three colours was randomly generated and repeated for examples 1-10. The reason for changing the colours between the examples was to reduce the risk of the respondents remembering the colour-to-data mapping between examples.

As an exmaple: for the progression of images 1-10, three lists are generated: a colour progression ["green", "blue", "red], a random list of numbers from the number of examples (21) [3, 14, 4, 10, 16, 2, 7, 9, 20], and selection of legend types [[first 5: no-legend{fixed}, random order of 4 remaining types], last 5: random order of 4 remaining types, [no-legend{fixed}]], then these were combined as ["3-green-no-legend", "14-sampled-blue", "4-red-clustered", "10-green-annotated", ..., "20-green-no-legend"]
}. Each response and a number of interactions by the respondents were sent back to the back-end server and persisted in a data base as the respondents went through the task. The response was picked using a number picker covering the range of the data (kept constant to 0-100). The initial value of the slider was random, and the respondents were required to change the response before being able to progress to the next example image.

3. After progressing through the example images, the respondents were asked about how helpful they considered the different legend types were for their task. This information was also sent back and persisted to the data base together with IDs of the respondents.

More information, example images of the user interface and text of the HTML markup can be found in Appendix B.

The survey data was collected between Jan 9th and Feb 6th 2021, using convenience sampling. Many of the 34 respondents are employees at the Volvo Car Corporation Asia Pacific Head Quarter in Shanghai, China.

The inital plan was to put the survey online using cloud-technologies for wider distribution. During the testing stage, it was however clear that there was a need to check that the instructions were understood to reduce the variation in the data. From this a more qualitative methodology was chosen where I was present in the room and asked the respondent if the task was understood after reading the instructions described in stage 1. of the list above. 

This resulted in a more consistent study, where the same laptop was used by all data collection, and the lighting conditions were possible to be kept consistent.


## *Data Analysis*

### *Data and variable description*


```{r import-data-and-clean, cache=TRUE, echo=FALSE}
source("./../data_read_in.R")
responses_an = df_from_db()


########################################
# Data Cleaning
########################################
error_uuids = c(
  "db5cf00b-448f-47cf-842c-8d237f6cf45b",
  "f0e25470-5bf5-4363-b75f-bc9f704dc9b1"
)


responses_an = responses_an %>% filter(!(uuid %in% error_uuids))

responses_an$uuid = as.factor(responses_an$uuid)
responses_an$maptype = as.factor(responses_an$maptype)
responses_an$colour = as.factor(responses_an$colour)
responses_an$legend = as.factor(responses_an$legend)

responses_an_viz = responses_an
responses_an_viz$legend <- factor(responses_an_viz$legend, levels = c("headline", "checkered", "sampled", "clustered", "annotated"))


```



**Treatment**

*LEGEND* - Each of the legend types, as the only part of visualisaion that is not randomized. Have values of no legend (headline), checkered, clustered, sampled and annotated corresponding to section \ref{legends}.

```{r response-error-table, echo=FALSE, message = FALSE}

tbl_summary(
  responses_an %>%
    select(legend)
) %>%
  modify_header(label = "*Total*") 

```

Each of the 34 respondent completed the full survey, resulting in 340 responses to the data-progression, and as seen they are distributed equally among the legend types.


**Dependent Variables**

*PERCEPTION ERROR* distance (delta) between the response value to the actual value of the location of the marker per visualization. 

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

```{r response-error-visualization, fig.pos = "h", fig.cap = "Perception error by visualization category", echo=FALSE}
ggplot(responses_an_viz, aes(legend, percept_error, color=legend)) + 
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  stat_summary(fun.data="mean_sdl", fun.args = list(mult = 1), 
               geom="pointrange", color="black") + 
  theme_minimal() +
  theme(legend.position="none") + 
  ylab("Perception Error")
```

As seen in Figure `r figure_number`, the mean is centered around 0, with some visually differences in distribution between the headline (no legend) category having the largest variance and more outliers. The figure has the mean indicated by a black dot, with 1 standard deviation in each direction shown by a range for each category.


*|PERCEPTION ERROR|* Absolute perception error - for linear modeling there is a need to look at mean-differences between categories, as simply using the error will result in all categorical differences being statistically insignificant.

 ```{r echo=FALSE}
figure_number = figure_number + 1 
```

As seen by Figure `r figure_number`, the same data visualized as boxplots with an absolute transformation of the perception error, shows the mean (visualized as a black point) differeing between the categories - highest for no legend, and lowest for annotated legend. As can also be seeen there are some outliers (visualized as dots outside above the "whiskers") for almost all categories \footnote {
defined for box plots as 1.5 times the interquartile  range [the height of the boxes] outside the interquartile range
} , except for headline/no legend, where the variance of the data is as quite large.


```{r abs-response-error-visualization, fig.cap = "Box plot - absolute error by visualization category", echo=FALSE}

ggplot(responses_an_viz, aes(legend, percept_error_abs, color=legend)) +
  geom_boxplot()  +
  stat_summary(fun="mean", geom="point", color="black") + 
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Absolute Perception Error")

```

*ACCEPTANCE* - objective valuation by participants on the usefulness of each legend type. On the scale 1-5, with responses of "No opinion" treated as missing values/Unknown. 

```{r acceptance-table, echo=FALSE, message = FALSE}
tbl_summary(
  responses_an_viz %>%
    select(uuid, acceptance, legend) %>%
    distinct() %>% select(-c(uuid)),
  by = legend
) %>%
  add_n() %>% # add column with total number of non-missing observations
  modify_header(label = "**Legend**") %>% # update the column header
  bold_labels()


```
Table **TODO-INSERT TABLE NUMBER** indicates that the highest *ACCEPTANCE* scores are for annotated and sampled legend types. More statistical comparison to follow in the results section. 
**TODO: WHY 164, if 33 would respond = 165, also for all 34, would be 170 + comment on this!!!**

**Independent/Control Variables Candidates**

*SUBMIT TIME* - time in seconds from the rendering of the example until the subject submitting their response. In study design expected to be a proxy for uncertainty.

*INPUT CHANGES* - Number of times the respondent changes their answer with input-slider before submitting. In study design expected to be a proxy for uncertainty.

*HOVER EVENTS* - Events of hovering the mouse cursor over the example image logged and sent back to back end. In study design expected to be a proxy for uncertainty.

Due to the more quantitative study design where the experiment designer was present in the room, it became clear that all these uncertainty-proxy variables would be likely to contain limited amount of signal. *SUBMIT TIME* varied because of incoming phone calls, the need to ask questions about the design types and many other external factors, *INPUT CHANGES* varied due to how the respondents chose to input their responses and work with the slider generally, and *HOVER EVENTS* did not vary much within-subjects, and wasn't consistently used as a visual guide.

Plots for these variables can be found in Appendix C.

*PROGRESSION* - For each respondents, numbering of the examples they were shown from 1-10. Later to be used to check for learning and/or fatigue effects.

*COLOUR* - Visualization presented having opacity data layer mapped colour channel as pure red, green or blue. From visual inspection, not any strong difference between the colours (see Appendix C). 


### *Hypothesis testing models*

Most of the research questions are centered around how different legend design choices affects errors and acceptance.

For *PERCEPTION ERROR* the average value is centered around 0, but the distribution variance is of interest. Most statistical models are constructed to compare in particular mean values between categories, which would not be able to give any indication of differences between these values. It would be possible to create a non-parametric test using bootstrapping of almost any test statistics (Efron, 1979). However, for simplicity Bartlett's test \footnote{
See https://en.wikipedia.org/wiki/Bartlett%27s_test and http://www.sthda.com/english/wiki/compare-multiple-sample-variances-in-r for comparisons of standard statistical test for variance differences
} allow testing of multiple variance differences between multiple groups. **TODO: REFERNECE**

The Bartlett test will be conducted for both the full data including the headling/no-legend type as baseline, and a subset to see if variances differs in a statistically significant way between the different legend types, using checkered legend type as a baseline, as the type having the largest variance test statistic of the sample.

Bartlett's test assumes normality, so the Shapiro-Wilk test **TODO: REFERENCE** are first to be conducted in order to see if the normality-assumption holds. This will be done for all legend types combined, with headline/no-legend removed, as well as for the subsample of each legend type separately.  

For *|PERCEPTION ERROR|*, *ACCEPTANCE* and other questions where a mean-comparison is able to be of relation to the reseach questions \footnote{
Including *SUBMIT TIME*, *INPUT CHANGES* and *HOVER EVENTS*, which will have the results presented in Appendix **TODO - APPENDIX LETTER**
} are examined using regular OLS-techniques (linear regression). Of importance for interpretable results is the baseline for these models (intercept), for ease of understanding p-values and test statistic differences. 

The baseline OLS-estimates are retained using 

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} LEGEND_{i} + \varepsilon_{i}
\end{equation*}

and

\begin{equation*}
ACCEPTANCE_{i} = \alpha + \beta_{1} LEGEND_{i} + \varepsilon_{i}
\end{equation*}

To account for individual fixed effects\footnote{Certain respondents are likely to have different average correctness due to within-subject variations}, this is in case of statistically significant findings also tested using individual fixed effects (FE)-models **TODO: REFERENCE**. This can be thought of as including separate intercept-variables for each respondent using dummy variables for each respondent\footnote{The statistical analysis software may choose to estimate this using other techniques, such as entity-demeaned OLS for computational efficiency https://www.econometrics-with-r.org/10-3-fixed-effects-regression.html}.

\begin{equation*}
|PERCEPTION ERROR|_{ij} = \alpha + \beta_{1} LEGEND_{i} + \gamma_{2} RESPONDENT2{j} + \gamma_{3} RESPONDENT3{j} + ... + \gamma_{n} RESPONDENTn{j} + \varepsilon_{ij}
\end{equation*}

The individual effects observed was not expected to be as significant as before the study was conducted, due to a more controlled environment for data collection as described in section \ref{data-collection-ui}.

However, the FE models can easily cluster standard errors, to affect statistical significance also for other estimators in the model.


### *Robustness checks*

"Time fixed effects", is considered to be added to the models to check for learning and/or fatigue effects as the respondent went through the progression of tasks. In order to see if this is of value to pursue further, first a simple OLS-model is tested to see if there are any indications of

Given that the subjects were given no feedback on the correctness of their responses during the progression, learning effects are likely to be negligible.

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION_{i} + \varepsilon_{i}
\end{equation*}

An alternative model will be run on the subset of only the first and last example subset (headline/no-legend), to see if these categories differ significantly.

\begin{equation*}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION LAST_{i} + \varepsilon_{i}
\end{equation*}

The results from these test will feed into the possible need to do subsample analysis using only e.g. the first 5 responses of each respondent.

Subset analysis using only the respondents that are likely to be the most engaged or skilled, as defined as the respondents with below median average *|PERCEPTION ERROR|* is to be conducted to see if the results differ in a significant way for those respondents.



# *Results*

## *PERCEPTION ERROR*

```{r baseline-data-processing, echo=FALSE, message = FALSE}
########################################
# ERROR AND DISPLAY TYPES
# 1. legends -> opacity mapping perception? baseline = no legend
# 2. alternative legend choices vs. standard? baseline = checkered
# 3. legend to data proximity-reduction|contextualised -> error? baseline = checkered
########################################

# Data sets
no_legend_baseline = responses_an
no_legend_baseline$legend = relevel(responses_an$legend, ref = "headline")
checkered_baseline = responses_an
checkered_baseline = checkered_baseline %>% filter(legend != "headline")
checkered_baseline$legend = relevel(checkered_baseline$legend, ref = "checkered")

```


### *Normality tests*

```{r normality-testing, echo=FALSE, message = FALSE}


test_normality <- function(df, legend_type){
  if(legend_type=="combined"){
    perception_error_only_df = responses_an %>% select(percept_error)
  } else if(legend_type=="headline_removed"){
    perception_error_only_df = responses_an %>% filter(legend!="headline") %>% select(percept_error)
  }
  else{
    perception_error_only_df = responses_an %>% filter(legend==legend_type) %>% select(percept_error)
  }
  normality = shapiro.test(perception_error_only_df$percept_error)
  normality_results = ifelse(normality$p.value > 0.05, paste(legend_type, "NORMALity not rejected"), paste(legend_type, "NON-NORMAL"))
  # print(normality_results)
  return(normality)
}

# Sensitive to normality
legend_types = c(
  "combined",
  "headline_removed",
  "headline",
  "sampled",
  "clustered",
  "checkered",
  "annotated"
)


normality_list <- data.frame()

for(legend in legend_types){
  normality_results <- test_normality(responses_an, legend)
  
  normality_list <- rbind(normality_list, 
                           data.frame(legend=legend, 
                             test_statistics = normality_results$statistic[[1]],
                             p_value = normality_results$p.value[[1]])
  )
}

normality_list$legend <- c("Combined", "Headline Removed", "Headline", "Sampled", "Clustered", "Checkered", "Annotated")

names(normality_list) <- c("Sample", "Shapiro Wilk Test Statistics", "P-Value")

```

As shown in the table below **TODO: TABLE NUMBERING**, all the p-values for the Shapiro-Wilk test is above 0.05, meaning the the null of normality is not rejected for any of the tests run. In otheer words, we can assume that the data is normally distributed. 

```{r normality-testing-table, echo=FALSE, message = FALSE, results='asis'}

library(stargazer)
stargazer(normality_list,type='latex', summary = FALSE, header = FALSE)


```


### *Bartlett test of homogeneity of variances*


```{r processing-variance-ordering, echo=FALSE, message = FALSE}

# check which category has the highest variance
varince_ordering <- no_legend_baseline %>%
  group_by(legend) %>%
  summarize(sd = sd(percept_error)) %>%
  arrange(desc(sd))


varince_ordering$legend <- plyr::revalue(varince_ordering$legend, c(
  headline="Headline", 
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))



names(varince_ordering) <- c("Legend", "Standard Dev.")

varince_ordering$Legend <- as.character(varince_ordering$Legend)
varince_ordering$`Standard Dev.` <- round(varince_ordering$`Standard Dev.`, 1)
```

```{r processing-variance-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(varince_ordering,type='latex', summary = FALSE, header = FALSE)
```


Table **TODO: TABLE NUMBERING** shows that the example with no-legend/headling shows largest variance/standard deviation, followed by checkered, sampled, clustered and annotated legend types.

In the statistical tests the headline type is used first as baseline, to see if these differences are statistically significant, then a subset removing all the headline data points from the sample is used, and see if the variance differ significantly.

```{r bartlett-variance-models, echo=FALSE, message = FALSE}

bartlett_model_all = bartlett.test(percept_error ~ legend, data = no_legend_baseline)
# ifelse(
#   bartlett_model_all$p.value < 0.05,
#   "Variance SIGNIFICANT difference",
#   "Variance NO significant difference"
# )


bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_df <- data.frame(
  "Sample" = c(
    "All, including Headline",
    "Excluding Headline"
  ),
  "Test Statistic" = c(
    bartlett_model_all$statistic,
    bartlett_model_against_checkered$statistic
    ),
  "P Value" = c(
    bartlett_model_all$p.value,
    bartlett_model_against_checkered$p.value
  )
)

```

```{r bartlett-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(bartlett_df,type='latex', summary = FALSE, header = FALSE)
```

As seen from Table **TODO: TABLE NUMBERING**, there is a statistically significant difference in variance for the sample including the Headline type (p-val < 0.05), but that significance does not remain when excluding the headline/no-legend category. This indicates that there is not much need to look into two-way comparisons between the legend types.

For estimations of size of differences between groups transformation of *PERCEPTION ERROR* to absolute terms is analyzed instead.

## *|PERCEPTION ERROR|*

Visual inspection of whether there are clear difference in heterogenity in *|PERCEPTION ERROR|* for both legend types and between respondents are seen in **TODO: FIGURE NUMBERING** in Appendix C (bars including 95% confidence intervals). The confidence intervals are largely overlapping for all types except when not including any legend (Headline), which is in line with results from variance testing. For respondent variation, there are large variations, but still a large fraction that have overlapping confidence bounds.

```{r abs-perception-error-models, echo=FALSE, message = FALSE}
library(plm)
library(lmtest)
library(zoo)

# Absolute error - simple OLS
## 1. legend useful at all
### Baseline
baseline_legend_of_use <- lm(percept_error_abs ~ legend,
                     data = no_legend_baseline)

### FE = robustness
fe_legend_of_use <- plm(percept_error_abs ~ legend,
                          data = no_legend_baseline,
                          index = c("uuid"),
                          model = "within")
# print summary using robust standard errors
# coeftest(fe_legend_of_use, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_legend_of_use)


## 2. Legend vs standard = checkered
## 3. annotated or contextualized also answered
### Baseline
standard_baseline_difference <- lm(percept_error_abs ~ legend,
                             data = checkered_baseline)

### FE = robustness
fe_standard_difference <- plm(percept_error_abs ~ legend,
                            data = checkered_baseline,
                            index = c("uuid"),
                            model = "within")
# print summary using robust standard errors
# coeftest(fe_standard_difference, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_standard_difference)


```

```{r abs-perception-error-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  baseline_legend_of_use,
  fe_legend_of_use,
  standard_baseline_difference,
  fe_standard_difference,
  title="Regression Results",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(2,2),
  dep.var.labels = c("|PERCEPTION ERROR|")
  )


```


The regression results shown in Table **TODO: TABLE NUMBERING** show that the results are consistent with the variance tests, where model (1) and (2) estimates are relative to the reference category of no-legend/headline. All other legend types display lower errors, and are all statistically significantly lower than the errors for headline, with the annotated type being having the lowest average error of `r round(fe_legend_of_use$coefficients[[1]], 2)` lower errors than headline type on the range that is between 0-100.

Where checkered is used as reference category, all estimates are as done when choosing that as reference due to both the research question formulation and the data distribution are showing lower estimated errors, but none of those results are statistically significant.

The individual effects have not affected the estimates, but have some higher standard errors. Everything that is significant for simple OLS is also significant to roughly the same extent for the FE models. **CHECK ALTERNATIVES -> SAY AND MAKE SURE ROBUST STANDARD ERRORS ARE INCLUDED**


## *ACCEPTANCE*

**TODO, check n-issue if any!!! from subset filtering - seems okay, nrow(acceptance_data_unique)=170**

```{r processing-acceptance, echo=FALSE, message = FALSE}
# NEED TO USE DATA WITHOUT DUPLICATES

acceptance_data_unique = responses_an %>%
    select(uuid, acceptance, legend) %>%
    distinct()

```

```{r mean-group-, echo=FALSE, message = FALSE}
# Generally by group, which highest
ordered_acceptance_criterias <- acceptance_data_unique %>%
  group_by(legend) %>%
  summarize(mean = mean(acceptance, na.rm = T)) %>%
  arrange(desc(mean))

ordered_acceptance_criterias$legend <- plyr::revalue(ordered_acceptance_criterias$legend, c(
  "headline"="Headline",
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))


names(ordered_acceptance_criterias) <- c("Legend", "Mean")
ordered_acceptance_criterias$Legend <- as.character(ordered_acceptance_criterias$Legend)
ordered_acceptance_criterias$Mean <- round(ordered_acceptance_criterias$Mean, 2)


```

```{r processing-acceptance-out, echo=FALSE, message = FALSE, results='asis'}
stargazer(ordered_acceptance_criterias,type='latex', summary = FALSE, header = FALSE)
```

Table **TODO: LOOK UP TABLE NUMBER** shows the mean for each legend type of the *ACCEPTANCE*-scores. As expected, having no legend at all (Headline) have the lowest acceptance scores, while the sampled and annotated legend types have average acceptance scores above 4 out of a maximum of 5.

Given the previous baseline categories still make sense, to check the statistical significance of these numbers compared to the Headline/no-legend type, as well as against the checkered type which imitates ArcGIS's legend choice.




```{r acceptance-data-processing-and-models, echo=FALSE, message = FALSE}

# Earlier baselines still makes rough sense
acceptance_data_unique_legend_baseline = acceptance_data_unique
acceptance_data_unique_legend_baseline$legend = relevel(acceptance_data_unique_legend_baseline$legend, ref = "headline")
acceptance_data_unique_checkered_baseline = acceptance_data_unique
acceptance_data_unique_checkered_baseline = acceptance_data_unique_checkered_baseline %>% filter(legend != "headline")
acceptance_data_unique_checkered_baseline$legend = relevel(acceptance_data_unique_checkered_baseline$legend, ref = "checkered")

# Models- all significant against no-legend (= preferred over)
acceptance_headline_baseline <- lm(acceptance ~ legend,
           data = acceptance_data_unique_legend_baseline)
# Annotated and sampled significantly preferred, clustered lower, but not significantly so
acceptance_checkered_baseline <- lm(acceptance ~ legend,
           data = acceptance_data_unique_checkered_baseline)

```

As seen from the regression results in Table **TODO: INSERT TABLE NUMBER**, all acceptance means are statistically higher than Headline category, and in comparison to the checkered legend type, both sampled and annotated legend types are statistically significantly considered more helpful from the respondents. The clustered legend type has a lower acceptance score compared to the checkered legend, but not statistically significantly so.

The results were quite clear, and no strong reasons to believe that there are factors on individually controllable level that would alter the significance levels, no other models seemed necessary to control for these results through FE-models.


```{r acceptance-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  acceptance_headline_baseline,
  acceptance_checkered_baseline,
  title="Regression Results",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("ACCEPTANCE")
  )

```


## *Robustness tests*

Other variables that were collected as potential proxy variables for uncertainty (*SUBMIT TIME*, *INPUT CHANGES* and *HOVER EVENTS*), but exhibited high variability and large outliers to external factors did show some significant results as shown in Tables **TODO: TABLE NUMBERS** in Appendix C. Many variables are significant compared to Headline legend types, but quite likely those outliers are coming from first interaction with the survey responses being for the headline type.

To check for clear learning or fatigue effects that would merit subsample analysis, an OLS model was run on *|PERCEPTION ERROR|* against the linear progression variable (discrete values expressed as integers 1-10). Another model using only the subsample of the first and last images, both of Headline type, to check if the respondents learned to memorize the opacity data mapping without use of legends was also included.


```{r progression-model-checks, echo=FALSE, message = FALSE}

# Correlation error with progression
# learning effects as progression or getting tired of task... (INDIVIDUAL+PROGNUM)
progression_effect_mod <- lm(percept_error_abs ~ prog,
                             data = responses_an)
# Nothing significant, not going to take this into account - or do analysis using e.g. first 5 images only



# Baseline 1 vs baseline 10 - difference? - SUBSAMPLE ONLY 1st and 10th!!!
progression_effect_mod_baseline_check <- lm(percept_error_abs ~ factor(prog),
                             data = responses_an %>% filter(prog %in% c(1, 10)))


```


The results can be found in Table **TODO: TABLE NUMBER**. As seen, neither the linear trend variable, nor the categorical variable for the last image in the progression were statistically significant. The estimate is even showing a slightly higher error for the last example on average compared to the first for the gathered sample of 68 (34 respondents, 2 examples per respondent).

These results indicates that not much ought to differ from reducing the sample to e.g. a subsample of the first 5 examples in the progression for each respondent.

```{r progression-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  progression_effect_mod,
  progression_effect_mod_baseline_check,
  title="Regression Results",
  header = FALSE,
  type='latex',
  covariate.labels=c("ProgressionLinear","Progression Last","Constant/Reference"),
  column.labels = c("Linear trend", "Subsample fist/last, Reference=First"),
  column.separate = c(1,1),
  dep.var.labels = c("|PERCEPTION ERROR|")
  )

```

Another robustness-test was introduced to see if there could be clear differing results between the samples with legends in the responses that were on average more accurate in their task - i.e. removing outliers.

A subsample of the 50th percentile on average more accurate respondents were created, and the same models as for the full sample using **|PERCEPTION ERROR|** as dependent variable was run. 



```{r models-outlier-removal-preprocessing, echo=FALSE, message = FALSE}

# Respondents with smaller average errors (outlier removal) - top 50th percentile

num_respondents_chosen = (responses_an$uuid %>% unique() %>% length())/2

arranged_respondents_by_error = responses_an %>% group_by(uuid) %>%
  summarize(mean_abs_error = mean(percept_error_abs)) %>%
  arrange(mean_abs_error)

resp_low_errors = arranged_respondents_by_error[1:num_respondents_chosen,] %>% select(uuid)

resp_low_errors_df = responses_an %>% filter(uuid %in% resp_low_errors$uuid)
responses_an_accurate_viz = responses_an_viz %>% filter(uuid %in% resp_low_errors$uuid)

```

Visual inspection as can be found in Figure **TODO: INCLUDE FIGURE NUM**, there is still a wide distribution of values, where headline and clustered legend types have the higher average absolute error, while the other types both have  a number of high error outliers.

```{r models-outlier-removal-plot, echo=FALSE, message = FALSE}
# Absolute
# Not really anything that sticks out
ggplot(responses_an_accurate_viz, aes(legend, percept_error_abs, color=legend)) +
  geom_boxplot()  +
  stat_summary(fun="mean", geom="point", color="black")

```


```{r models-outlier-removal-model-definition, echo=FALSE, message = FALSE}

# For completeness
no_legend_baseline_accurate = resp_low_errors_df
no_legend_baseline_accurate$legend = relevel(no_legend_baseline_accurate$legend, ref = "headline")
checkered_baseline_accurate = no_legend_baseline_accurate
checkered_baseline_accurate = checkered_baseline_accurate %>% filter(legend != "headline")
checkered_baseline_accurate$legend = relevel(checkered_baseline_accurate$legend, ref = "checkered")

# Absolute error - simple OLS
## 1. legend useful at all
### Baseline
baseline_legend_of_use_accurate <- lm(percept_error_abs ~ legend,
                             data = no_legend_baseline_accurate)
# summary(baseline_legend_of_use_accurate)
### FE = robustness
fe_legend_of_use_accurate <- plm(percept_error_abs ~ legend,
                        data = no_legend_baseline_accurate,
                        index = c("uuid"),
                        model = "within")
# print summary using robust standard errors
# coeftest(fe_legend_of_use_accurate, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_legend_of_use)


## 2. Legend vs standard = checkered
## 3. annotated or contextualized also answered
### Baseline
standard_baseline_difference_accurate <- lm(percept_error_abs ~ legend,
                                   data = checkered_baseline_accurate)
# summary(standard_baseline_difference_accurate)
### FE = robustness
fe_standard_difference_accurate <- plm(percept_error_abs ~ legend,
                              data = checkered_baseline_accurate,
                              index = c("uuid"),
                              model = "within")
# print summary using robust standard errors
# coeftest(fe_standard_difference_accurate, vcov. = vcovHC, type = "HC1")
# Nonrobust -> summary(fe_standard_difference)

```

As seen in Table **TODO: TABLE NUMBER**, which is corresponding to Table **TODO: TABLE NUMBER** for this subsample, all estimates are showing statistically lower absolute errors compared to Headline category, except Clustered legend type, which is not any longer statistically significant. Checkered is still kept as reference category in the models where Headline types have been excluded, even though it is not the category with the lowest **|PERCEPTION ERROR|**. Here there is no difference in the significance after removing the less accurate respondents, with no statistically signficant results.


```{r models-outlier-removal-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  baseline_legend_of_use_accurate,
  fe_legend_of_use_accurate,
  standard_baseline_difference_accurate,
  fe_standard_difference_accurate,
  title="Regression Results - above 50th percentile accuracy",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(2,2),
  dep.var.labels = c("|PERCEPTION ERROR|")
  )


```


# Discussion

**TODO: NOT INCLUDED YET - COMPARISON TO OTHER PARTS OF LITERATURE, CAN SAY, LITTLE PUBLISHED ON THIS PARTICULAR QUESTION...**


The results shows that in this type of visualization types, there is an influence of legend choices - as to if legends are included or not. We see both a lower variance and more accurate results when there is a legend compared to when no legend is present. 

However, there were no clear results indicating differences in response accuracy depending on the type of legend, and there was quite a large variation in response accuracies in the sample overall. Notably as well was that user selection and other proxy variables were not well-suited to act as a proxy for respondent-confidence or uncertainty.

There was a worry if keeping the data in the same range between examples would lead the respondents to stop using the legends after getting used to/internalizing the data mapping. Without this the errors would not be on the same scale, so it was deemed more suitable, and instead a post-collection testing of progression results was to be relied on in addition to changing the colours between each example. Given that there was a consistently higher error for the Headline/no-legend examples, and actually slighltly higher errors for the last example compared to the first, this was unlikely to have affected the validity of the study.

The strongest results were perhaps from the user preferences/acceptance of the different legend types, where both Sampled and Annotated (more contextualized and higher placement proximity to data) were preferred to the "industry standard" as in the imitation of the Checkered legend type.

A common feedback given by respondents was that it would be useful to have larger legends. The current legend size was chosen to imitate ArcGIS opacity legend as closely as possible, and to be identical in size between legend types.

In order to control the setting for data collections, the examples were somewhat contrived. 

\begin{enumerate}
  \item Today, rather few data complex visualizations are lacking annotation or possibilities for interaction.
  \item All data area mappings were strictly rectangular in shape to allow for easy automated placement of the Annotated legend types.
  \item Only one opacity data layer is included. Multiple and possibly overlapping opacity layers would result in compounding opaqueness. This seems like a good design choice for data encoding to avoid more than one data opacity layer.
  \item The map data as base layer served more as a context, and realistic use case in terms of how data is viewed in common map applications. This layer was kept constant, and may not be representative of many other areas, and it is not clear how opacity mapping and legend are working for less complex (e.g. maps/backgrounds with less/no colours) or more complex backgrounds (e.g. aerial/satellite imagery).
\end{enumerate}

None of these factors, except from the last, ought to have strong implications for the external validity of the study, but ought to still be noted.

The sampling was non-random convenience sampling. This was as earlier discussed in order to keep other factors as consistent as possible. The number of engineers was high among the respondents, but it seems difficult to speculate if this would skew the results in any meaningful way.  



<!-- TODO: Migrate references!!! -->
<!-- Other literature on Perception with clutter/backgrounds/overlays -->
<!-- Other literature on legends... -->
<!-- # References

Adelson, Edward (1993). ‘Perceptual Organization and the Judgment of Brightness’. Science, 262, p2042-2044
ArcGIS (2020a) Create a custom visualization using Arcade [Online]. Available at: https://developers.arcgis.com/javascript/latest/sample-code/visualization-arcade/index.html (Accessed: 13 Sept 2020)
Berry, Lisa (2016) 6 Easy Ways to Improve Your Maps [Online]. Available at: https://www.esri.com/arcgis-blog/products/mapping/mapping/6-easy-ways-to-improve-your-maps/#:~:text=%206%20Easy%20Ways%20to%20Improve%20Your%20Maps,sometimes%20become%20distracting%20and%20tear%20your...%20More%20 (Accessed: 13 Sept 2020)
Brehmer, M. Munzner, T (2013) ‘A Multi-Level Typology of Abstract Visualization Tasks’. IEEE Transactions on Visualization and Computer Graphics, 19(12), p 2376 - 2385
Brewer, Cynthia (2006). ’ Basic Mapping Principles for Visualizing Cancer Data Using Geographic Information Systems (GIS)’. American Journal of Preventive Medicine, 30(2S).
Cairo, Alberto (2016) The Truthful Art:Data, Charts, and Maps for Communication. Berkeley, CA: New Riders.
Ciechanowski, Bartosz (2019). Alpha Compositing [Online]. Available at: https://ciechanow.ski/alpha-compositing/ (Accessed: 20 Feb 2021)  
Chan, MY, Wu, YC, Mak, WH, Chen, W (2009). ‘Perception-Based Transparency Optimization for Direct Volume Rendering’. IEEE Transactions on Visualization and Computer Graphics, 15(6).
Cleveland, WS, McGill, R (1985) ‘Graphical Perception and Graphical Methods for Analyzing Scientific Data’. Science, 229(4716), p828-833.
Correa, Carlos D, Ma Kwan-Liu (2008). ‘Size-based Transfer Functions: A New Volume Exploration Technique’.  IEEE Transactions on Visualization and Computer Graphics, 14(6).

Dykes, J., Wood, J. and Slingsby, A. (2010). ’Rethinking Map Legends with Visualization’. IEEE Transactions on Visualization and Computer Graphics, 16(6), p.890-899.

Ee, Shaun (2020). Back to school with Tencent and friends - Health surveillance goes into schools as some classes resume [Online]. Available at: https://technode.com/2020/04/28/health-code-back-to-school-with-tencent-and-friends/ (Accessed: 13 Sept 2020)

Efron, Bradley. (1979). Bootstrap methods: Another look at the jackknife. The Annals of Statistics. 7 (1): 1–26. doi:10.1214/aos/1176344552.

Grant, Robert (2019) Data Visualization – Charts, Maps and Interactive Graphics. Boca Raton, FL: CRC Press.
Jaemin Jo, Frédéric Vernier, Pierre Dragicevic, Jean-Daniel Fekete. ’A Declarative Rendering Model for Multiclass Density Maps’. IEEE Transactions on Visualization and Computer Graphics, Institute of Electrical and Electronics Engineers, 2019, 25 (1), pp.470-480.
Kim, YS, Walls ,LA, Krafft , P, Hullman, J (2017a). A Bayesian Cognition Approach to Improve Data Visualization. ACM Conference (Conference’17). ACM, New York, NY, USA.
Kim, YS, Reinecke, K, Hullman, J (2017b). Explaining the Gap: Visualizing One’s Predictions Improves Recall and Comprehension of Data. 2017 CHI Conference on Human Factors in Computing SystemsMay 2017 Pages 1375–1386
Metelli, Fabio (1974) ‘The Perception of Transparency’. Scientific American, 230(4), p90-99.
Midtbø, Terje (2007). ‘Advanced Legends for Interactive Dynamic Maps’. 23rd International Cartographic Conference, Moscow
Munzner, Tamara (2015) Visualization Analysis and Design. Boca Raton, FL: CRC Press.
Perlin, Ken (1985) ‘An Image Synthesizer´. Computer Graphics, 19(3), p287-296
Roth, RE, Woodruff AW, Johnson, ZF (2010). ’Value-by-alpha maps: An alternative technique to the cartogram’. The Cartographic Journal, 47(2) p130–140.
Tufte, Edward (2001). The Visual Display of Quantitative Information. 2nd edition. Cheshire, CO: Graphics Press.
Ware, Colin (2020) Information Visualization – Perception for Design. 4th edition. Camebridge, MA: Elsevier.

-->

<!-- # Appendices -->
\newpage

\section{Appendices} \label{appendices}


\subsection{Appendix A - example generations and technology choices}\footnote{All code available at https://github.com/Tille88/thesis-map-generation}

A number of layers was produced for the sample images and extracted programmatically as PNG-files. The choices made can easily be described in combination with these layer types.

\subsubsection{Element description}

*Base Map*

The javascript library OpenLayers\footnote{https://openlayers.org/} was used to pull down a background map to a large\footnote{larger than 1,600*1,600 pixels, and later downsampled due to not knowing usage at the data generation stage.} HTML5 Canvas element (web-browser raster-API). The area was chosen arbitrarily as Greenwich, London - and the zoom-level was chosen manually to get an area with diverse set of background colours representing water, buildings and other infrastructure. The default OpenStreetMap visualisation type was used.

This image data was then extracted and stored locally in order to be able to work with ease without internet connection, and when adding other layers it was pulled into the HTML5-Canvas from disk. 

*Opacity Data Layer*

In order not to have respondents making use of pre-existing knowledge or assumptions to the data given the common background of London, continuous data was generated using Perlin-noise (Perlin, 1985)\footnote{The implementation used to generate the noise was https://github.com/p5py/p5/blob/master/p5/pmath/rand.py }.

All generated data was normalized to have the lowest value to be 0.0, and highest being 1.0 - mapping to the linear data mapping for examples shown to respondents as being between 0 and 100.

Two different types were generated: one with-out falloff to the edges of the data area, only using pure Perlin noise within the area (see figure No falloff), and the other one making sure there was a more centralized higher data area, with decreasing values towards the edges of the rectangular data area (figure falloff).


![No falloff](./img/no-falloff.png){width=40%}

![No falloff](./img/falloff.png){width=40%}

The noise data was put into the opacity value for a canvas image-element, with 1 of the 3 RGB-colour channels set to the max value (e.g. for red, canvas per that pixel was rgba(255, 0, 0, opacityValue)).

In the following step, the base layer and map data layer was merged using the ctx.drawImage() of the HTML5 Canvas-API\footnote{This was also tested manually iterating over both the raster values of the base map and the data layer and combining them according to the Canvas opacity specification, but after seeng the results being identical, the simpler API was used}.

*Marker*

A marker was designed using SVG and overlayed at a randomized location within the data-extent. This was merged into the Canvas raster representation, and the value of the data later at the location was stored together with other metadata for each visualization to JSON.

![Marker](./img/marker.png){width=40%}

*Legends*

All legends were using the Canvas API, in order to add text, background of legend and merge with data legend in a similar fashion as done between base map and data layer. Some specific points that are non-obvious described below:

**Checkered legend type** - Following the ArcGIS design, a mid-level grey was used to generate a checkered-pattern as background.

**Sampled legend type** - 5 different rectangular parts of the base map data of the size of the legend was extracted and chosen randomly for the sample images.

**Clustered legend type** - Using kMeans clustering algorithm implementation in the base R language, was used to extract 10 "colour centers" representing common colours of the base map. These were then displayed as stripes behind the legend.


\subsubsection{Data extraction procedure}

Heavily relying on randomization for size of map area/perlin noise seed and marker location, Node.js and a headless browser was used in order to automatically extract 21 different examples in all combinations of: 

\begin{itemize}
    \item fully red, green, blue colour channel
    \item 5 legend types
\end{itemize}

These Canvas representations were exposed as PNGs and stored to disk together with metadata in JSON. The resulting examples were 21 examples * 3 colour types * 5 legend types = 315 examples.






\newpage


\subsection{Appendix B - User interface and data serving setup}\footnote{All code for front- and backend code can be found at https://github.com/Tille88/thesis-front-end and https://github.com/Tille88/thesis-back-end respectively.}

\subsubsection{Backend setup}

The backend logic used was quite simple: for each respondent/session, a identifier for the session is generated through a UUID, and a progression following the logic described in section \ref{data-collection-ui} was sent back to the front-end. Then the front-end requested these images from the equivalent of a file-server that contained the sample images.

The Node.js framework Express was used to make this functioning quickly and the interactions and responses sent back from the front-end was persisted in a MongoDB NoSQL database. Due to not having to run this online, this was done locally on the computer used for data collection.

\subsubsection{Frontend setup}

All front-end UI was created with HTML5, CSS and basic Javascript, shown in a Chrome web browser.

From the user-perspective, the user was first shown an introduction page giving some background and instructions as shown in the images below.

![Introduction page start](./img/intro1.png){width=40%}
![Elements - base map introduction](./img/intro2.png){width=40%}
![Elements - data layer introduction](./img/intro3.png){width=40%}

![Elements - legends introduction](./img/intro4.png){width=40%}
![Elements - marker introduction](./img/intro5.png){width=40%}

![Elements - response element introduction](./img/intro6.png){width=40%}

Following the introduction, and a verbal check on understanding the task, the session was initialized, receiving the progression of examples chosen randomly from the backend.

![Example UI](./img/exampleui.png){width=40%}


At the load of the image, a timer was started, and a number of events (hovering over image, response changes - with the current value of the input, location of hover, time of event, etc.) was logged locally. At the time of submission, all these event data, the time until submission and the final response value was sent back and persisted server-side. This was repeated 10 times.

The user was not able to go back once an answer had been submitted, and the "submit button" was not active until the user had updated the value from the initially random value in the range of 0-100.

![Submit inactive](./img/submit_inactive.png){width=40%}

![Submit active](./img/submit_active.png){width=40%}

Following the full 10 examples, the users were presented with acceptance questions as seen from images below:


![Acceptance top](./img/acceptance1.png){width=40%}

![Acceptance bottom](./img/acceptance2.png){width=40%}

All answers were sent back to the server and persisted.

After final submission a "Thank you screen" including virtual confetti indicated the end of the session and task.

![End of task indication](./img/end_screen.png){width=40%}


\newpage

\subsection{Appendix C - Plots and Tables}\footnote{All analysis code and data for reproducing results can be found at https://github.com/Tille88/thesis-data-analysis}


```{r submit-time-visualization, fig.cap = "Submit time", echo=FALSE}

# Submit time
ggplot(responses_an_viz, aes(legend, submitTime / 1000, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Seconds until Submit") +
  xlab("Legend")

```

```{r input-changes-visualization, fig.cap = "Input changes", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, inputChanges, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Number of Input Changes before Submit")

```


```{r hover-events-visualization, fig.cap = "Hover events", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, hoverEvents, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(legend.position="none") +
  ylab("Number of Hover Events before Submit")

```


```{r colour-visualization, fig.cap = "Hover events", echo=FALSE}


ggplot(responses_an_viz, aes(colour, percept_error_abs, color=colour)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.5) +
  scale_color_manual(values=c("blue", "green", "red")) +
  stat_summary(fun="mean", geom="point", color="black") +
  ylab("Absolute Perception Error")+
  xlab("Colour") +
  theme_minimal() +
  theme(legend.position="none")
  

```




```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(gplots)
plotmeans(percept_error_abs ~ legend, main="Heterogeineity across legend types", data=no_legend_baseline)
plotmeans(percept_error_abs ~ uuid, main="Heterogeineity across respondents", data=no_legend_baseline, connect = F, n.label = F)


# PLOTS
# TABLE
# https://thatdatatho.com/2018/08/20/easily-create-descriptive-summary-statistic-tables-r-studio/
```



```{r interaction-variable-modelling, echo=FALSE, message = FALSE}

# As shown earlier - a lot of variance, not a lot of signal
# Outliers likely affecting, simple OLS only

# Time until submit -> likely because of first interaction as legend
time_submit_all <- lm(submitTime / 1000 ~ legend,
                             data = no_legend_baseline)
# Between those with legend, no statistically significant difference difference compared to baseline
time_submit_w_legend <- lm(submitTime / 1000 ~ legend,
                  data = checkered_baseline)


# inputChanges - only on first interaction, nothing significant vs. checkered
input_changes_all <- lm(inputChanges ~ legend,
           data = no_legend_baseline)
input_changes_w_legend <- lm(inputChanges ~ legend,
           data = checkered_baseline)

# hoverEvents - no significant difference, high std errors, order of baseline category will not matter
hover_events_all <- lm(hoverEvents ~ legend,
           data = no_legend_baseline)
hover_events_w_legend <- lm(hoverEvents ~ legend,
           data = checkered_baseline)

```

```{r time-submit-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  time_submit_all,
  time_submit_w_legend,
  title="Regression Results - Time to submit",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("SUBMIT TIME")
  )

```


```{r input-changes-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  input_changes_all,
  input_changes_w_legend,
  title="Regression Results - Input changes",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("INPUT CHANGES")
  )

```

```{r hover-events-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  hover_events_all,
  hover_events_w_legend,
  title="Regression Results - Hover events",
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("HOVER EVENTS")
  )

```





