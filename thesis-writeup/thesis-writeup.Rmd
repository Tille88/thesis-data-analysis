---
output: 
  # pdf_document:
  bookdown::pdf_document2:
    includes:
      before_body: frontpage.tex
    # citation_package: natbib
    keep_tex: true
    toc: false
    fig_caption: true
    latex_engine: xelatex
    number_sections: true  
header-includes:
  - \usepackage{hyperref}
  - \pagenumbering{gobble}
  - \usepackage{pdfpages}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{rotating}
  - \usepackage{acronym}
  - \usepackage{float}
  - \floatplacement{figure}{H}
mainfont: Times New Roman
font-family: Times New Roman
linestretch: 1.5
fontsize: 12pt
papersize: a4
geometry: margin=2cm

bibliography: citation/references.bib
csl: citation/elsevier-harvard.csl


---


\newpage
\pagenumbering{roman}
\setcounter{page}{5}
\tableofcontents 
\newpage

\listoffigures

\listoftables

\setlength{\parindent}{0cm}
\setlength{\parskip}{0.5cm}

\newpage

\section*{List of abbreviations} 

\begin{acronym}
  \acro{API}{Application Programming Interface}
  \acro{CSS}{Cascading Style Sheets}
  \acro{ESRI}{Environmental Systems Research Institute, Inc.}
  \acro{GIS}{Geographic Information System}
  \acro{HTML5}{Hypertext Markup Language, 5th major version}
  \acro{OLS}{Ordinary Least Squares}
  \acro{RGBA}{Red, Green, Blue, Alpha}
  \acro{SVG}{Scalable Vector Graphics}
  \acro{UI}{User Interface}
\end{acronym}



<!-- Empty page for print formatting reasons -->
\newpage 
\thispagestyle{empty}
\ 

\newpage


```{r, setup, include=FALSE}
# knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
library(dplyr)
library(ggplot2)
library(stargazer)
library(tidyr)
library(gplots)
library(plm)
library(zoo)


# library(lmtest)
# library(GGally)
# library(corrplot)
# library(gtsummary)
# library(mongolite)
# library(stringr)
# library(Hmisc)
```




\pagenumbering{arabic} 


\section{Introduction} \label{introduction}

\subsection{Opacity/transparency in GIS visualizations} \label{introduction-opacity-gis}

Having a base map for geographic context and the need to visualize other geospatial data as an overlay layer on top of that geographic context results in the background being partly or fully concealed. To show part of the geographical context, a common method is to reduce the opacity of the overlay data layer. This is also sometimes referred to as increasing the inverse of opacity; transparency. 

@Ware20 highlights some issues of using low opacity layers in his seminal work on information visualization. In particular, he describes that colours and objects that the user perceives become composites of the merged layers:

> *In many visualization problems, it is desirable to present data in a layered form. This is especially common in geographic information systems (GISs). So that the contents of different layers are simultaneously visible, a useful technique is to present one layer of data transparently over another; however, there are many perceptual pitfalls in doing this. The contents of the different layers will always interfere with each other to some extent, and sometimes the two layers will fuse perceptually so that it is impossible to determine to which layer a given object belongs [@Ware20, p. 217].*

An illustrative example of how choosing lower opacity can show more of the base map, while higher opacity can make the geographic context difficult to see can be found in Figure \ref{fig:low-high-opacity-example}. In this figure, the same data is mapped to 2 different ranges of opacity values. Figure \ref{fig:low-opacity-example} has the data mapped between opacity values of 0.2-0.6, while Figure \ref{fig:high-opacity-example} starts at 0.6 and ends at fully opaque (0.6-1.0). With the low opacity, there is more of the base layer colour influencing the colour that is displayed to the user, which in turn could make decoding the data more difficult.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/opacity_0_2_to_0_6.png}
         }
         \caption{Opacity mapping 0.2-0.6}
         \label{fig:low-opacity-example}
     \end{subfigure}
     
     \vspace{0.4cm}
     
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/opacity_0_6_to_1.png}
         }
         \caption{Opacity mapping 0.6-1.0 (1.0=fully opaque)}
         \label{fig:high-opacity-example}
     \end{subfigure}
     
     \caption{Low and high opacity overlay examples}
      \label{fig:low-high-opacity-example}

\end{figure}
 
This difficult decoding task can be made easier by interaction in web-based GIS systems. In those systems, a user can interact with the visualization to get values from e.g. tooltips on hover of the mouse over parts of the map. For static visualizations and on touch-screen devices (such as smartphones or tablets) mouse-hovering is not possible. A legend is therefore often used both in interactive and static web-GIS visualizations.

This study aims to look at the efficiency of legend designs in visual opacity-to-value decoding tasks. The study also investigates which of 4 legend designs the participants of the study perceive as most helpful in the decoding task.

This is done using a web-based graphical user interface and having a focus group of people with medium to high previous experience using map services estimate the value of a marker by using different legend designs. 

A user study was chosen due to being able to capture the responses of participants in a controlled environment. It would be possible to get more responses by publishing it on the Internet. However, to be able to control if parts of the instructions were unclear to the participants were instrumental in collecting clean data. 

Additionally, with a smaller focus group, you can also easily collect responses on the impressions of the legend designs. In comparison to more qualitative methodologies, such as in-depth interviews, you can get standardized and measurable results.  The choice of a user study does come with some caveats in what population the chosen sample of users is expected to be representative of. For this study, having users with some familiarity with reading maps or at least being heavily exposed to maps through navigation applications were chosen to ensure that common map elements and representations were familiar, and the minor design changes could be easily distinguishable.

The legend design of the enterprise GIS company Esri's "ArcGIS online" tool [@ArcgisIndex; @ArcgisPlatform] was together with the absence of a legend used as baselines to evaluate alternative legend designs against.

ESRI's “ArcGIS Online” examples [@ArcgisBrightOnline; @ArcgisDarkOnline] using transparency are showing a legend that has a checkered background with white and mid-grey values (Figures \ref{fig:opacity-dark-background} and \ref{fig:opacity-bright-background}). This legend representation of transparency lacks any context of the visualization's base layer/map and is always displayed as a checkered mid-grey background under the opacity mapping. 



\begin{figure}
     \centering
     
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/online_example_1.png}
         }
         \caption{Opacity data mapping and legend with colour scale, highlighted with a red bounding box}
         \label{fig:opacity-dark-background}
     \end{subfigure}

    
    \vspace{0.4cm}
    
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/online_example_2.png}
         }
         \caption{Opacity data mapping and legend, highlighted with a red bounding box}
         \label{fig:opacity-bright-background}
     \end{subfigure}
        \caption{ArcGIS opacity legend and data mapping (ArcGIS, n.d.a, n.d.b)}
        \label{fig:arc-gis-opacity-legend-and-mapping}
\end{figure}
 

\subsection{Research questions} \label{introduction-res-questions}

- **RQ1** - Do legends increase the accuracy of decoding opacity-mapped values over knowing only the range of the data being mapped (i.e. when no legend is included)?
- **RQ2** - How is decoding accuracy affected by legend designs that introduce more context from the base map or reduce the distance from legend to opacity-encoded data?
- **RQ3** - Do subjects perceive legend designs with more context from the base map as more helpful in decoding opacity mapped values?
- **RQ4** - How are decision times and user selection behaviour affected by legend designs? 



<!-- # Background -->
\section{Background} \label{background}

The ubiquity of web mapping and the increased ease of creating customized maps for distribution on the web makes online maps a part of most people's daily life. @Panko18 estimates that 77% of smartphone owners regularly use navigation apps, where map visualizations are a key part of every interface. Google maps had over one billion monthly users in 2014, representing 41% of Internet users
worldwide [@Veenendaal17]. According to a site measuring the presence of different web technologies online, maps were present in around 25% of the top 100,000 sites in July 2021, of which Google maps accounted for 74% [@Builtwith21]. In the list of frequently used map technologies, there are also other alternatives, such as MapBox, ArcGis, and Leaflet.js. It is not easy to customize the base map that comes from centrally controlled servers, resulting in the use of colour scales and semi-transparent heat map visualizations as a way to add data to map visualizations.

This frequent exposure to maps creates a multiplying effect in the exposure of even less frequently seen map types in terms of impressions (how many people see even less common visualizations). Lower opacity visualizations\footnote{mapping of data directly to opacity or mapping of data to other channels, such as a colour scale in combination with lowered opacity to reduce occlusion.} are suitable for visualizations of e.g. high-impact events such as typhoons/hurricanes. There are very few previous studies on the perception accuracy of such map designs as will be outlined in section \ref{literature}.

\subsection{Related studies} \label{literature}

Literature that intersects with the scope of this study can be found in a few different research fields.

In the area of perception and data visualization studies, there is a long history of low-level studies measuring the ability of humans to correctly perceive visual data encodings, such as position, length, angle, circle area, hue, etc. [@Cleveland828]. 

There are also studies on perception effects from context, such as luminosity perception of a grey area depending on surrounding areas [@Adelson2042]. For higher-level tasks, @Tufte01 exemplifies attempts to bring these smaller encodings together to describe good data visualization practices. This corpus of literature usually tries to create design guidelines and best practices for data visualizations. 

These efforts to quantify design choices and data encoding methods have been criticized, where e.g. Muzner [-@brehmermuzner13] describes a gap between these low-level tasks and high-level tasks in the data visualization literature. In recent years, researchers have commonly attempted to bridge the gap towards other scientific areas [@Kim17; @Kim19], incorporating mental models, uncertainty, and Bayesian prior beliefs of subjects into the perception task. The goal in this literature is to bridge the combination of the lower-level tasks and relate them to efficient user experiences through e.g. preattentive processing - meaning differentiating objects of interest to the viewer by giving it different encoding (e.g. giving data points of interest emphasis by encoding them in different colours than the other data points) [@Treisman85; @Findlay98; @Wolfe04].

A similar discourse is also present in the field of map visualizations. Cognitive cartography blends the fields of cognitive methods with how maps are used. This cover perception of e.g. colours, the need for users to keep information in their long term or working memory. The way this has commonly been studied through low-level identification and decoding tasks and creating a controlled environment, where subjects may not interact with maps in a similar way they would outside of the experimental setting, has been criticized in, as for the larger literature on data visualization outline above [@Montello02].


Studies that cover transparency directly or indirectly for 2D data visualization are rare. @Jo19 briefly mentions opacity: 

> *To scale scatterplots, several approaches have been proposed, such as adaptive opacity [15, 30, 32] and aggregation [13, 53]. However, adaptive opacity does not scale well with the number of categories since multiple categorical colors become ambiguous when blended[..].*

For map and perception studies, colour choices in a cartographic context is a large corpus, with @Brewer06 having provided colour palette tools commonly used throughout data visualization from studying colour use from a cartographic/GIS perspective. These online tools help in creating suitable colour scales taking into consideration factors such as contrast, usability for people with colour blindness, etc. [@Brewer03]. The colour schemes/palettes provided by the tool have been extensively used also in other data visualization fields outside of map-making. 

@Brewer97 have also studied the use of colour scales, which is an alternative to using only a single colour as in this study. @Brewer97 states that "*Cartographers have long discouraged the use of spectral, or rainbow, color schemes on thematic maps of quantitative geographic data, though such color use is common in GIS and scientific visualization.*, but goes into detail on how spectral schemes can be useful if designed with care. 

This level of care comes when using choosing multiple colours to encode data, as the distance between different colours is perceptually ordered only for short sections, but not for the full spectrum. @Ware20 exemplifies the issue: 

> *This can be demonstrated by the following test. Give someone a series of gray paint chips and ask them to place them in order. They will happily comply with either a dark-to-light ordering or a light-to-dark ordering. Give the same person paint chips with the colors red, green, yellow, and blue and ask them to place them in order, and the result will be varied. For most people, the request will not seem particularly meaningful. [@Ware20, p. 128].*

Colour is necessarily one of the most common ways to encode and communicate information in maps and other visualization types, and the design of colour schemes for different use cases is a field where many variations and use cases are studied. In the greater literature on how to represent uncertainty in maps [e.g. @Cheong16; @Leitner00], there is also the creation of dedicated colour schemes for as in @Seipel17's study on the colour map design when assessing flood risk using maps.

Colours and transparency interact by definition, as the output per pixel is a calculated composite of the colour of the transparent object and the colour of the items behind [@Porter84]. In the context of 3D models, and the ability to perceive objects clearly this is a major and well-studied problem. @Seipel20 have shown that in this context that distinct colours (evaluated using not only hue, but also perceptual properties) when fully opaque becomes difficult to distinguish when transparency is increased due to colour blending.  

Other studies have looked at transparency/opacity more generally in distinguishing objects, finding that in computer-based 3D modelled scenes reducing the transparency number of levels in the scene helps users in identification tasks  [@Wang17].


Targeted literature on legends is often highly tied to different visualization types. In a walk-through of proper data visualization practices, Evergreen and Metzner suggest that there is clear importance of data-proximity of legend usage and placement: 

> *Because human eyesight has only a narrow range of focus, graphics should be placed very near their associated text (Malamed, 2009; Ware, 2012). [...] if a legend is required in the visualization, it should be so near the corresponding data points that no eye movements are needed to relate the two.* [@Evergreen13]. 

A large part of the literature studying legends in the context of maps has a clear design focus. @Peterson99 introduced an interactive legend that allowed users to make changes to the map through the legend interactions, finding that users can get a better understanding of maps through the interactions. Other design studies have looked at fairly narrow questions related to legend designs for maps. Examples of such design studies have been using additional diagrams in combination with classic legends as a supplement to choropleth maps [@Cromley09; @Cromley06; @Chien19], and in other cases substituting the legend for a frequency histogram altogether [@Kumar04].

In a more holistic view on legend usage/design within web GIS, @Cybulski16 analyzed if animated maps that are published on the internet have designs that follow long-established guidelines within cartography. @Cybulski16 finds that these maps do follow the guidelines in general, but also that they are adapting to new technologies and user circumstances.

No intersection between legends and transparency, either in GIS-context or elsewhere, has been found in the literature review. This is largely supported in the literature study "Grouping Rules for Effective Legend Design" by @Zhe17 that aimed to find whether there were cartographic rules for effective legend designs *"[...]it is found that only one study was dedicated to the building of cartographic rules for effective legend design"*.

@Kiik17 evaluates some visualization alternatives in the cartographic design of polygons, transparency being one of the alternatives, together with outlines/boundaries, hatches (texture within the polygons) and icons. The results with examples of multiple polygons the hatches design was more effective along many of the metrics gathered using eye-tracking technology, but the transparency design was preferred by most of the subjects. This study used multiple overlayed polygons with low opacity, creating a colour blending effect between the polygons.


# Methodology

Four different map legend designs, as well as a version without legend, were generated and then shown to respondents through a web user interface. Using this interface, the task for the respondents was to estimate the value of the opacity-mapped overlay at the location of a marker.

The generation of the examples allowed all parts of the designs apart from the legends to be kept constant. This was done to control the exposure to the treatment (different legend designs, or absence of legends). 


\subsection{Legend design and generation} \label{legends}

\subsubsection{Legend design types} \label{legends-design}

The map designs were all static 2D maps shown using web technologies. The subjects were shown different legend designs and were asked to estimate the value at the location of a marker. The data was represented as a spatially distributed phenomenon mapped to opacity in a polygon overlaid on a base map, as in the ArcGIS online maps described in section \ref{introduction-opacity-gis}.

The 4 legend designs and the version without legend, are presented below, together with some justifications – where all legends and data mappings are linear in the opacity/alpha channel. This ought to introduce fewer complications than data-encoding using colour channels due to a linear encoding of the alpha/opacity-channel in the RGBA-model\footnote{RGBA = Red, Green, Blue, Alpha}:

>  *It’s worth pointing out that unlike the color components which are often encoded using a non-linear transformation, alpha is stored linearly – encoded value of 0.5 corresponds to alpha value of 0.5 [@Ciechanowski19]*


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/cropped_headline_highlight.png}
         }
         \caption{Baseline 1 - No legend ["Headline"]}
         \label{fig:headline}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/cropped_checkered_highlight.png}
         }
         \caption{Baseline 2 – ArcGIS Imitation ["Checkered"]}
         \label{fig:checkered}
     \end{subfigure}
          \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/cropped_sampled_highlight.png}
         }
         \caption{sampled context ["Sampled"]}
         \label{fig:sampled}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/cropped_clustered_highlight.png}
         }
         \caption{clustered colour bands ["Clustered"]}
         \label{fig:clustered}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \fbox{
         \includegraphics[width=\textwidth]{img/cropped_annotated_highlight.png}
         }
         \caption{Annotated Outline ["Annotated"]}
         \label{fig:annotated}
     \end{subfigure}
    \caption{Visualization legend types and design with no legend (enlarged legend for clarity, highlighted in blue)}
    \label{fig:legend-types}
\end{figure}
 
 



\emph{Baseline 1 No legend ["Headline" in tables], (Figure \ref{fig:headline})} - Only having a title indicating the range of values. Used to test the correctness of visual decoding by subjects without legend. Used as the first and last example in the progression presented to the subjects.


\emph{Baseline 2 – ArcGIS Legend Imitation ["Checkered" in tables], (Figure \ref{fig:checkered})} - The design is used as a baseline for models that explore if alternative legend choices are helping in decoding values compared to the current "industry standard". 

One likely downside making it more difficult for subjects to accurately decode the data values is the lack of background context in the legend, i.e. the colours in the legend do not interact with a coloured background as it does in the visualization. The remaining legend design types are all designed to combine the map base-layer background information into the legend.
 

\emph{Legend with sampled context ["Sampled" in tables], (Figure \ref{fig:sampled})} - Sampling a rectangular area of the base layer as background to the opacity mapping in the legend.


\emph{Legend with clustered colour bands ["Clustered" in tables], (Figure \ref{fig:clustered})} - Using the most common colours in the base layer as a background for the legend. In the Clustered legend design, the 10 most common colours of the background map\footnote{Using kMeans clustering algorithm implementation in the base R language, was used to extract 10 "colour centers" representing common colours in the base map.} were displayed as strips/columns in a vertical manner behind the opacity data mapping. This way the most prevalent colours in the map provide a context for data-encoding in the legend. 


\emph{Annotated Outline ["Annotated" in tables], (Figure \ref{fig:annotated})} - The legend placement may heavily affect the users' ability to keep the information in near memory while moving the eyes back and forth between visualized data and the legend. In an attempt to move the information closer to the data an outline legend was placed next to the data polygon. The legend is contextualized by having the legend opacity viewed directly on top of the base map layer.

\subsubsection{Map example design and generation} \label{legend-generation}

Several layers and elements were produced for the sample map images\footnote{All code available at https://github.com/Tille88/thesis-map-generation}. These building blocks of the example maps are described below.


*Base Map* - The Javascript library OpenLayers\footnote{https://openlayers.org/} was used to download a background map to a large\footnote{larger than 1,600*1,600 pixels, and later downsampled due to not knowing usage at the data generation stage.} HTML5 Canvas element (web-browser raster-API). The area was chosen arbitrarily as Greenwich, London. The zoom level for the map was chosen manually to get an area with a diverse set of background colours representing water, buildings, and other infrastructure.


*Opacity Data Layer* - In order not to have respondents making use of pre-existing knowledge or assumptions, continuous data was generated using simulated Perlin-noise [@Perlin85]\footnote{The implementation used to generate the noise was https://github.com/p5py/p5/blob/master/p5/pmath/rand.py }. The Perlin-noise family of algorithms are an alternative to other ways to generate data, such as using random noise, pure trigonometrical functions or statistical distributions. Due to the unpredictable and highly natural look of Perlin-noise, it is commonly used in computer-generated imagery (e.g. in computer-generated art and in video games). 

All generated data were normalized to have the lowest value to be 0, and the highest being 100.

Two different types were generated: one using pure Perlin noise within the area (see Figure \ref{fig:no-falloff}), and the other one using "fall-off": having higher data values in the center of the data area, with decreasing values towards the edges (Figure \ref{fig:falloff}).


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/no-falloff.png}
         \caption{No falloff}
         \label{fig:no-falloff}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/falloff.png}
         \caption{With falloff}
         \label{fig:falloff}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/marker.png}
         \caption{Marker}
         \label{fig:marker}
     \end{subfigure}
        \caption{Map elements}
        \label{fig:map-element-discussion}
\end{figure}



The Perlin noise data was put into the opacity value for a canvas image element, with 1 of the 3 RGB-colour channels set to the max value, e.g. for red, canvas per that pixel was rgba(255, 0, 0, opacityValue). The same data overlay mapped to the different colours are shown in Figure \ref{fig:map-colors}.


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/color_red.png}
         \caption{Red opacity data layer}
         \label{fig:color-red}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/color_green.png}
         \caption{Green opacity data layer}
         \label{fig:color-green}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/color_blue.png}
         \caption{Blue opacity data layer}
         \label{fig:color-blue}
     \end{subfigure}
        \caption{Identical data and marker positions for all colors}
        \label{fig:map-colors}
\end{figure}


*Marker* - A marker was designed using the vector-based SVG-API in the browser and overlaid at a randomized location within the data-extent (see Figure \ref{fig:marker}). This was merged into the Canvas raster representation, and the value of the data layer at the location was stored.

*Legends* - All legends were created using the Canvas API, similarly as was done for the base map and data layer. The different legend types have been described in detail in section \ref{legends-design}.


\subsubsection{External validity and limitations of example designs} \label{legends-validity}

To control the setting for the data collection, the examples were somewhat contrived. 

\begin{itemize}
  \item Today, rather few complex data visualizations are lacking annotation or possibilities for interaction.
  \item All data area mappings were strictly rectangular to allow for easy automated placement of the Annotated legend types.
  \item Only one opacity data layer was included. However, this seems like a good design choice as multiple and possibly overlapping opacity layers would result in compounding opaqueness.
  \item To keep the designs similar for the decoding task, the size of the legends were chosen to imitate the ArcGIS design. Verbal feedback was given by one respondent that it would be useful to have larger legends.
  \item The base map layer was kept constant, and may not be representative of other areas. It is not clear how opacity mapping and legend are working for less complex (e.g. maps/backgrounds with less/no colours) or more complex backgrounds (e.g. aerial/satellite imagery).
\end{itemize}

None of these factors, except the last, ought to have strong implications for the external validity of the study.


\subsection{Data collection} \label{data-collection}

\subsubsection{Procedure and user interface (UI)} \label{data-collection-ui}


A front-end UI was developed using HTML5, CSS, and Javascript so that it could be displayed using any web browser\footnote{All frontend code can be found at https://github.com/Tille88/thesis-front-end}. In this system, the respondents were taken through three distinct stages:

**View 1 - Introduction of task (Part shown in figure \ref{fig:intro-page})**

Introduction with instructions of the decoding task and the UI. The elements of the map (base map, data layer, marker, and legend) were shown visually together with the UI elements used to submit responses (slider, submit button). The respondents were explicitly asked to use the legends to estimate the value of the data layer at the location of a randomly generated marker. 

Following the introduction, and a verbal check on understanding the task, the session was initialized by clicking a button in View 1.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/intro2.png}
         \caption{Elements - base map introduction}
         \label{fig:elem-basemap}
     \end{subfigure}
     \hfill
     \hfill
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/intro4.png}
         \caption{Elements - legends introduction}
         \label{fig:elem-legends}
     \end{subfigure} 
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/intro6.png}
         \caption{Elements - response element introduction}
         \label{fig:elem-response}
     \end{subfigure} 
        \caption{Introduction of task}
        \label{fig:intro-page}
\end{figure}


**View 2 - Visual decoding of 10 examples, exposing the legend designs in semi-random order**

Using the web interface of \emph{View 2}, the users visually estimated the value at the location of a marker within the overlay data area.

Ten images picked by the back-end server from a large number of generated example combinations\footnote{
All possible combinations of colours (3, being red/green/blue), random data and marker location variations (21 examples that were generated through automated scripts) and designs (5, made up from 4 legend designs + 1 no legend), resulting in 3 * 21 * 5 = 315 different examples
} were presented to the respondent one example at a time. 

The progression of the images always started and ended with the baseline of Headline/no-legend type, and had the order of the 4 legend variations picked uniformly random for the example progression images 2-5 and 6-9 respectively. 

The colours of the data layer were available in pure red, green, and blue for all examples. For the first image in the progression, a combination of these three colours (e.g. ["Blue", "Red", "Green]) was randomly generated and repeated for examples 1-10. The reason for changing the colours between the examples was to reduce the risk of the respondents remembering the colour-to-data mapping between examples.

The progression for each respondent was generated using a combination of colour, data example (data layer and marker location), and legend types using a simple algorithm.

As an example: for the progression of images 1-10, three lists were generated: 

1. A random list of numbers from the number of examples (21), i.e. between 1-21, where each number could only be picked once per progression. \newline
Example list of 10 elements (i): [3, 14, 4, 10, 16, 2, 7, 9, 12, 20]


2. A colour progression, e.g. ["green", "blue", "red"] that was circulated: green, blue, red, then starting with green again. \newline
Example list of 10 elements (ii): ["green", "blue", "red", "green", "blue", "red", "green", "blue", "red", "green"]


3. A selection of legend types always starting and ending with Headline/no-legend: [\textbf{first 5 elements:} headline, random order of 4 legend types, \textbf{last 5 elements:} random order of 4 legend types, headline]. \newline
Example list of 10 elements (iii): ["no-legend", "sampled", "clustered", "annotated", "checkered", "clustered", "checkered", "sampled", "annotated", "no-legend"]


4. Then these lists (1-3) were combined into a progression, corresponding to the file names for the generated example images \newline
Example list of of combined lists (i, ii, and iii): ["3-green-no-legend", "14-blue-sampled", "4-red-clustered", "10-green-annotated", "16-blue-checkered", "2-red-clustered", "7-green-checkered", "9-blue-sampled", "12-red-annotated", "20-green-no-legend"]

The response (estimate of the value at the marker) and some interactions by the respondents were persisted as the respondents went through the generated progression of 10 examples. The response for each example was picked using a slider in the UI covering the range of the data (0-100). The initial value of the slider was random, and the respondents were required to change the response before being able to progress to the next example image (UI example shown in Figure \ref{fig:progression-ui}).

At the load of the image, a timer was started, and some events (mouse hovering over image, response changes of the slider, time of event relative to the example loading, etc.) were stored in the browser's memory. At the time of submission of an example, all event data, the time from load until submission, and the final response value was persisted server-side. This was repeated 10 times.

The user was not able to go back once an answer had been submitted, and the "submit button" was not active until the user had updated the value from the initially random value in the range of 0-100 (see Figure \ref{fig:submit-inactive} and \ref{fig:submit-active}).

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/exampleui.png}
         \caption{Progression UI}
         \label{fig:progression-ui}
     \end{subfigure}
     
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/submit_inactive_highlight.png}
         \caption{Submit inactive - highlight in red}
         \label{fig:submit-inactive}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/submit_active_highlight.png}
         \caption{Submit active - highlight in red}
         \label{fig:submit-active}
     \end{subfigure}
        \caption{Progression UI}
        \label{fig:pregression-ui-combined}
\end{figure}

The reason for choosing 10 examples per respondent was to ensure a large amount of data could be collected with a limited number of respondents. During testing prior to data collection, this amount of examples were established to be possible to complete within less than ten minutes, and not be very tiring for the respondent. 

Having each respondent being able to see each legend design twice, and have Headline/no-legend designs as first and last example, also allows easier testing of learning effects and being able to check if there is any major differences between not having a legend after being exposed to many examples.

**View 3 - User acceptance**

After progressing through the 10 example images, the respondents were asked about how helpful they considered the different legend types were for their task. This information was also sent back and persisted to the database (Figure \ref{fig:acceptance-ui}).

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/acceptance1.png}
         \caption{Acceptance UI top}
         \label{fig:acceptance-top}
     \end{subfigure}
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/acceptance2.png}
         \caption{Acceptance UI bottom}
         \label{fig:acceptance-bottom}
     \end{subfigure}
        \caption{Acceptance UI}
        \label{fig:acceptance-ui}
\end{figure}

After having to choose very precise numbers of a continuous scale between 0-100 for the prior step, a common web-form format and only five choices were chosen for user acceptance. These values can not be seen to be fully comparable, as they are more subjective and less precise than estimation errors, which can be measured to the decimal level. In other words, for each respondent, you get an ordinal scale, but it is not necessarily comparable between respondents. That is also the reason why each respondent had to give a value for each legend design to have a balanced data set for analysis.

\subsubsection{Paricipants in the study} \label{collection-info}

The survey data was collected between Jan 9th and Feb 6th, 2021, using convenience sampling. A sample of participants with moderate to good experience of map reading was considered ideal for a relatively small sample size, and to increase the chance of understanding the task quickly. Participants with a background in software development, UI/UX designers and employees of companies with map services were requested to participate. As the data was collected during a time with COVID-restrictions, most participants were either working or visiting the workplace of the author of the study, resulting in most of the 34 respondents being employees at the Volvo Car Corporation Asia Pacific Head Quarter in Shanghai, China, as well as vendors visiting the location.

All participants had at least a bachelor degree. Most hold Master degrees in technical disciplines, and one participant held a doctorate in mathematics.

The sample consisted of 7 women and 26 men. Approximately one-third of the participants work with geospatial data on a weekly basis, while most of the remaining participants spend significant time testing vehicles and planning field tests routes using maps similar to the base map of the study. All of the participants were in the age range of 25-50, with most below the age of 35.

The participants are on average expected to have better eyesight than the general population due to their relatively young age but are more likely to have some variation of colour blindness due to the high proportion of men participating in the study. Colour blindness or eyesight was not asked or tested during the data collection.

The initial plan was to put the survey online using cloud technologies for wider distribution. During the testing stage, it became clear that there was a need to check that the instructions were understood to reduce the variation in the data. Hence, a more qualitative methodology was chosen where the author was present in the room and asked the respondent if the task was understood after reading the instructions described in section \ref{data-collection-ui}. 

This resulted in a more consistent study, where the same laptop and Chrome web browser was used by all data collection, and the lighting conditions were kept consistent. All 34 respondents did therefore see the examples of the same size on an identical screen and using the same rendering software.

Due to each respondent submitting responses to 10 example images, the resulting analyzed data sample was quite large. During the collection phase, some base models were estimated repeatedly to evaluate the results of a larger sample. At around 20 participants the results stabilized, but data collection was continued until the pool of suitable participants at the Volvo office was exhausted.


## Data analysis

Multiple models are estimated to evaluate the effect of the independent categorical variable of legend design. The dependent variables are estimation errors and acceptance scores. A full list of variables is given in section \ref{variable-info}.

Outlined in section \ref{model-info}, Bartlett's statistical test is conducted to evaluate how the variance of the error are differing between the different legends. This is followed by Ordinary Least Squares (OLS) estimation with dependent variables that are not naturally centered around 0 - absolute estimation error and acceptance scores. Each legend design is encoded as separate dummy variables and estimated relative to a reference category that is chosen to answer the research questions.

Section \label{robustness-info} describes the use of control variables and subsamples to be able to test the robustness of the results in section \ref{model-info}. 

\subsubsection{Data and variable description} \label{variable-info}

```{r import-data-and-clean, cache=TRUE, echo=FALSE}
source("./../data_read_in.R")
responses_an = df_from_db()


########################################
# Data Cleaning
########################################
error_uuids = c(
  "db5cf00b-448f-47cf-842c-8d237f6cf45b",
  "f0e25470-5bf5-4363-b75f-bc9f704dc9b1"
)


responses_an = responses_an %>% filter(!(uuid %in% error_uuids))

responses_an$uuid = as.factor(responses_an$uuid)
responses_an$maptype = as.factor(responses_an$maptype)
responses_an$colour = as.factor(responses_an$colour)
responses_an$legend = as.factor(responses_an$legend)

responses_an_viz = responses_an
responses_an_viz$legend <- factor(responses_an_viz$legend, levels = c("headline", "checkered", "sampled", "clustered", "annotated"))


```


**Dependent Variables**

*PERCEPTION ERROR* - distance between the response value to the actual value of the location of the marker per visualization. 

*|PERCEPTION ERROR|* - absolute perception error. For linear modelling, there is a need to look at estimated mean differences between categories. Simply using *PERCEPTION ERROR* would result in all categorical differences being statistically insignificant unless the variance around 0 is heavily skewed.

*ACCEPTANCE* - objective valuation by participants on the usefulness of each legend type on a scale of 1-5. Opt-out responses of "No opinion" were allowed and treated as missing values in the statistical modelling. 

**Independent Variables**

*LEGEND* - categorical variable for each of the 4 legend types and no legend. The only part of visualization that is not randomized, and designed to be the treatment variable for which effects are to be estimated. Having values of Headline (no legend), Checkered, Clustered, Sampled, and Annotated, corresponding to section \ref{legends}.


Three variables were collected with the aim to find a good proxy variable for uncertainty:

*SUBMIT TIME* - time in seconds from the rendering of the example until the subject submitted their response.

*INPUT CHANGES* - number of times the respondent changes their answer with the input-slider before submitting. 

*HOVER EVENTS* - events of hovering the mouse cursor over the example image.

Two variables were collected as potential control variables:

*PROGRESSION* - for each respondent, the numbering of the examples in the progression from 1-10. Used to check for learning and/or fatigue effects.

*COLOUR* - visualization presented having opacity data layer mapped colour channel as pure red, green, or blue.


\subsubsection{Hypothesis testing models} \label{model-info}

For *PERCEPTION ERROR* the average value is likely centered around 0. Most statistical models are constructed to compare mean values between categories, which would not be able to give any indication of differences between the *PERCEPTION ERROR* for different legend designs. However, the distribution variance is of interest to estimate. 

The parametric Bartlett's test [@Snedecor89] is used to test if multiple categories all have equal variances. In the study, Bartlett's test is used to test if variances differ in a statistically significant way between the different legend types. This is done for both the full data including the Headline/no-legend type, as well as a subset excluding the Headline type. Significant results for the tests would indicate that the distribution of errors differs between any of the legend categories included in the sample. 

Due to that Bartlett's test assumes that data is normally distributed, Shapiro-Wilk tests [@Shapiro65] are first conducted to assert if the normality assumption holds. This will be done for all legend types pooled, with Headline removed from the pooled sample, as well as for the subsample of each legend type separately.  

For variables that are not centered around 0, *|PERCEPTION ERROR|*, *ACCEPTANCE*, *SUBMIT TIME*, *INPUT CHANGES* and *HOVER EVENTS*, are examined using regular OLS techniques. Due to the treatment variable *LEGEND* is categorical, each category is transformed into a separate dummy variable for each legend type. 

To avoid multicollinearity, the baseline legend categories are interpreted as the intercept $\alpha$-estimate, against which the $\beta$-estimates and corresponding test statistics are compared against.

The baseline OLS models estimated are of the form 

\begin{equation}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{i} LEGEND_{i} + \varepsilon_{i}
\end{equation}

and

\begin{equation}
ACCEPTANCE_{i} = \alpha + \beta_{i} LEGEND_{i} + \varepsilon_{i}
\end{equation}


\subsubsection{Robustness checks} \label{robustness-info}

OLS models are estimated to see if there are any indications of time effects as the respondent went through the progression of 10 examples. This could be in either direction due to learning and/or fatigue.

Learning effects are likely to be negligible. This is due to that the subjects were not given any feedback on the correctness of their responses during the progression, and that different legend designs and data overlay colours were rotated.

\begin{equation}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION_{i} + \varepsilon_{i}
\end{equation}

An alternative model is run on the subset of only the first and last example (Headline/no-legend) encoded as a dummy variable, to see if the response accuracy differed between the first and the last examples shown to respondents.

\begin{equation}
|PERCEPTION ERROR|_{i} = \alpha + \beta_{1} PROGRESSION LAST_{i} + \varepsilon_{i}
\end{equation}

The results from these tests also feed into the possible need to do subsample analysis using only e.g. the first 5 responses of each respondent.

Subset analysis using only the respondents that are likely to be the most engaged or skilled, as defined as the respondents with below-median average *|PERCEPTION ERROR|* are conducted to see if the results differ in a significant way.



# Results

## Summary statistics and exploratory graphs

**Treatment**

```{r response-error-table, echo=FALSE, message = FALSE}

error_tbl_summary <- responses_an %>%
  select(legend) %>%
  group_by(legend) %>%
  summarize(N=n())


error_tbl_summary$legend <- plyr::revalue(error_tbl_summary$legend, c(
  "headline"="Headline", 
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))

error_tbl_summary <- rbind(
  error_tbl_summary,
  data.frame(legend=c("Total"), N=sum(error_tbl_summary$N))
)


names(error_tbl_summary) <- c("Legend", "N")

error_tbl_summary$Legend <- as.character(error_tbl_summary$Legend)

```


Each of the 34 respondents completed the full survey, resulting in 340 responses to the data-progression, and as seen they are distributed equally among the legend types by design (Table \ref{tab:responses-summary}).

```{r response_tbl_summary_output, echo=FALSE, message = FALSE, results='asis'}

stargazer(
  error_tbl_summary,
  type='latex',
  rownames = FALSE,
  summary = FALSE, header = FALSE,
  label = "tab:responses-summary",
  title="Responses by legend type"
  )


```

**Dependent Variables**

*PERCEPTION ERROR* - As seen in Figure \@ref(fig:response-error-visualization), the mean is centered around 0 for all categories of *LEGEND*. There are some visually distinguishable differences in distribution between the Headline (no legend) category having the largest variance and more outliers compared to the examples with legends. The figure has the mean indicated by a black dot, with 1 standard deviation in each direction shown by a range-whisker for each category.

```{r response-error-visualization, fig.pos = "h", out.width = '55%', fig.align = "center", fig.cap = "Perception error by visualization category", echo=FALSE}
ggplot(responses_an_viz, aes(legend, percept_error, color=legend)) + 
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  stat_summary(fun.data="mean_sdl", fun.args = list(mult = 1), 
               geom="pointrange", color="black") + 
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) + 
  ylab("Perception Error") +
  xlab("Legend")
```

*|PERCEPTION ERROR|* - Figure \@ref(fig:abs-response-error-visualization) shows the same data as in Figure \@ref(fig:response-error-visualization) with an absolute transformation. The mean differs between the categories, being highest for Headline type, and lowest for Annotated legend type. The variance of *|PERCEPTION ERROR|* is quite large throughout. The vast majority of categories have absolute errors heavily clustered below 20, but there are outliers above 30 for all categories except Annotated. 


```{r abs-response-error-visualization, out.width = '55%', fig.align = "center", fig.cap = "Absolute error by visualization category", echo=FALSE}

ggplot(responses_an_viz, aes(legend, percept_error_abs, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
    stat_summary(fun.data="mean_sdl", fun.args = list(mult = 1), 
               geom="pointrange", color="black") + 
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) +
  ylab("Absolute Perception Error") +
  xlab("Legend")


```

*ACCEPTANCE* - Table \ref{tab:acceptancy-tally} indicates that the highest *ACCEPTANCE* scores are for Annotated and Sampled legend types. Headline/no-legend has the lowest acceptance scores.

```{r acceptance-table-process, echo=FALSE, message = FALSE}


acceptance_unique_df <- responses_an_viz %>%
    select(uuid, acceptance, legend) %>%
    distinct() %>% select(-c(uuid))

acceptance_spread_df <- acceptance_unique_df %>%
  group_by(acceptance, legend) %>%
  summarise(n=n()) %>%
  spread(legend, n, fill=0)

names(acceptance_spread_df) <- c("Acceptance", "Headline", "Checkered", "Sampled", "Clustered", "Annotated")


acceptance_spread_df <- acceptance_spread_df %>% replace_na(list(Acceptance = "No Opinion"))

```
```{r acceptance-table, echo=FALSE, message = FALSE, results='asis'}

stargazer(
  acceptance_spread_df,
  type='latex',
  rownames = FALSE,
  summary = FALSE, header = FALSE,
  label = "tab:acceptancy-tally",
  title="Acceptance response count cross-tabulation"
  )

```

**Independent/Control Variables Candidates**

In the data collection phase, it became clear that all the uncertainty-proxy variables would be unlikely to be valid proxies for respondent uncertainty. 

*SUBMIT TIME* varied because of incoming phone calls, the need to ask questions about the design types and many other external factors, *INPUT CHANGES* varied due to how the respondents used the slider\footnote{
Some respondents picked their response in one click, while other respondents clicked the slider to put it in focus, and then use the arrow keys to change the input value. In the former case, this would result in one *INPUT CHANGE*-event, and in the latter, there would be a separate event logged for each arrow key-press incrementing/decrementing the value by 0.1 each click.
}. *HOVER EVENTS* did not vary much within-subjects as the mouse cursor wasn't consistently used as a visual guide (some users did not use the mouse cursor, but kept it still while using fingers on the screen to make comparisons).

Plots for *SUBMIT TIME*, *INPUT CHANGES* and *HOVER EVENTS* (Figures \@ref(fig:submit-time-visualization), \@ref(fig:input-changes-visualization) and \@ref(fig:hover-events-visualization)) can be found in Appendix A.

*COLOUR* - From visual inspection, no strong difference between the colours red/green/blue examples were perceptible (see Figure \@ref(fig:colour-visualization) in Appendix A). Statistical estimates also had no statistical differences in *|PERCEPTION ERROR|* between the colours (Table \@ref(tab:models-color) in Appendix A), reducing the need to include colour as a control variable in any models.


## Dependent variable: PERCEPTION ERROR

```{r baseline-data-processing, echo=FALSE, message = FALSE}
########################################
# ERROR AND DISPLAY TYPES
# 1. legends -> opacity mapping perception? baseline = no legend
# 2. alternative legend choices vs. standard? baseline = checkered
# 3. legend to data proximity-reduction|contextualised -> error? baseline = checkered
########################################

# Data sets
no_legend_baseline = responses_an
no_legend_baseline$legend = relevel(responses_an$legend, ref = "headline")
checkered_baseline = responses_an
checkered_baseline = checkered_baseline %>% filter(legend != "headline")
checkered_baseline$legend = relevel(checkered_baseline$legend, ref = "checkered")

```


### Normality tests

```{r normality-testing, echo=FALSE, message = FALSE}


test_normality <- function(df, legend_type){
  if(legend_type=="combined"){
    perception_error_only_df = responses_an %>% select(percept_error)
  } else if(legend_type=="headline_removed"){
    perception_error_only_df = responses_an %>% filter(legend!="headline") %>% select(percept_error)
  }
  else{
    perception_error_only_df = responses_an %>% filter(legend==legend_type) %>% select(percept_error)
  }
  normality = shapiro.test(perception_error_only_df$percept_error)
  normality_results = ifelse(normality$p.value > 0.05, paste(legend_type, "NORMALity not rejected"), paste(legend_type, "NON-NORMAL"))
  # print(normality_results)
  return(normality)
}

# Sensitive to normality
legend_types = c(
  "combined",
  "headline_removed",
  "headline",
  "sampled",
  "clustered",
  "checkered",
  "annotated"
)


normality_list <- data.frame()

for(legend in legend_types){
  normality_results <- test_normality(responses_an, legend)
  
  normality_list <- rbind(normality_list, 
                           data.frame(legend=legend, 
                             test_statistics = normality_results$statistic[[1]],
                             p_value = normality_results$p.value[[1]])
  )
}

normality_list$legend <- c("Combined", "Headline Removed", "Headline", "Sampled", "Clustered", "Checkered", "Annotated")

names(normality_list) <- c("Sample", "Shapiro Wilk Test Statistics", "P-Value")

```

As shown in Table \@ref(tab:normality-test), all the p-values for the Shapiro-Wilk test are above 0.05, meaning the null of normality is not rejected for any of the tests run. In other words, we can assume that the data is normally distributed, no matter how it is separated by legend types, or if the statistical test is run on the full combined sample. 

```{r normality-testing-table, echo=FALSE, message = FALSE, results='asis'}

stargazer(
  normality_list,
  type='latex', 
  summary = FALSE, header = FALSE, 
  label = "tab:normality-test",
  title="Shapiro-Wilk normality tests"
  )


```


### Bartlett's test of homogeneity of variances


```{r processing-variance-ordering, echo=FALSE, message = FALSE}

# check which category has the highest variance
varince_ordering <- no_legend_baseline %>%
  group_by(legend) %>%
  summarize(sd = sd(percept_error)) %>%
  arrange(desc(sd))


varince_ordering$legend <- plyr::revalue(varince_ordering$legend, c(
  headline="Headline", 
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))



names(varince_ordering) <- c("Legend", "Standard Dev.")

varince_ordering$Legend <- as.character(varince_ordering$Legend)
varince_ordering$`Standard Dev.` <- round(varince_ordering$`Standard Dev.`, 1)
```

```{r processing-variance-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  varince_ordering,type='latex', 
  summary = FALSE, 
  header = FALSE,
  label = "tab:std-dev-by-cat",
  title="Error standard deviation by legend type"
)
```


Table \@ref(tab:std-dev-by-cat) shows that the Headline types show the largest variance/standard deviation for the error, followed by Checkered, Sampled, Clustered, and Annotated legend types.

In the statistical tests, the Headline type was used first as a baseline, to determine if these differences were statistically significant, then a subset removing all the Headline data points from the sample was used.

```{r bartlett-variance-models, echo=FALSE, message = FALSE}

bartlett_model_all = bartlett.test(percept_error ~ legend, data = no_legend_baseline)
# ifelse(
#   bartlett_model_all$p.value < 0.05,
#   "Variance SIGNIFICANT difference",
#   "Variance NO significant difference"
# )


bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_model_against_checkered = bartlett.test(percept_error ~ legend, data = checkered_baseline)

bartlett_df <- data.frame(
  "Sample" = c(
    "All, including Headline",
    "Excluding Headline"
  ),
  "Test Statistic" = c(
    bartlett_model_all$statistic,
    bartlett_model_against_checkered$statistic
    ),
  "P Value" = c(
    bartlett_model_all$p.value,
    bartlett_model_against_checkered$p.value
  )
)

```

```{r bartlett-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  bartlett_df,
  type='latex', 
  summary = FALSE, header = FALSE,
  label = "tab:bartlett-test",
  title="Bartlett's test results"
  )
```

As seen from Table \@ref(tab:bartlett-test), there is a statistically significant difference in the variance for the sample including the Headline type (p-value < 0.05). This indicates that when including the Headline category, the variance of the *PERCEPTION ERROR* is not the same for all categories.  

The significance does not remain when excluding the Headline category from the sample (p=0.33). The null of that the variance of the legend designs created for the study are the same can not be rejected, i.e. using Bartlett's test, the legend designs does not show differences in errors.

\subsection{Dependent variable: |PERCEPTION ERROR|} \label{subsection-perception-error}

```{r abs-perception-error-models, echo=FALSE, message = FALSE}

# Absolute error - simple OLS
## 1. legend useful at all
### Baseline
baseline_legend_of_use <- lm(percept_error_abs ~ legend,
                     data = no_legend_baseline)

## 2. Legend vs standard = checkered
## 3. annotated or contextualized also answered
### Baseline
standard_baseline_difference <- lm(percept_error_abs ~ legend,
                             data = checkered_baseline)

```


The way of reading of Table \@ref(tab:abs-perception-errors), as well as Tables \@ref(tab:ordered-acceptance), \@ref(tab:abs-perception-errors-accurate), \@ref(tab:models-time-to-submit), \@ref(tab:models-input-changes), \@ref(tab:models-hover-events) are identical. The dependent variable is displayed at the top corresponding to the variable on the left side of equation (1) in section \ref{model-info}. Two different models columns are listing which variable are used as reference  estimate $\alpha$ in equation (1). For each column the estimate for that category can be found under the "Constant/Reference" column. So for Model (1), the average error for the Headline design is `r round(baseline_legend_of_use$coefficients[[1]], 2)`. This also means that for model (2) where Checkered is the reference category, there is no estimate for the Checkered column.

The estimates for the other rows (Annotated, Checkered, Clustered and Sampled), corresponds the $\beta$-estimates in equation (1), and are relative to the reference category. In Model (1) in Table \@ref(tab:abs-perception-errors), Annotated has an average error relative to the headline category of `r round(baseline_legend_of_use$coefficients[[2]], 2)`, meaning that the actual estimated error is `r round(baseline_legend_of_use$coefficients[[1]], 3)`-`r round(baseline_legend_of_use$coefficients[[2]], 3)`=`r round(baseline_legend_of_use$coefficients[[1]]+baseline_legend_of_use$coefficients[[2]], 3)`.

The regression results in Table \@ref(tab:abs-perception-errors) are consistent with the variance tests, where model (1) estimates are relative to the Headline reference category. 

All other legend types display lower errors and are all statistically significantly lower than the errors for Headline. The Annotated type had the lowest average error of `r round(baseline_legend_of_use$coefficients[[2]], 2)` lower errors than the Headline design.

In model (2), where Checkered is used as a reference category (chosen due to both the research question formulation and the data distribution) the other designs are showing comparatively lower estimated errors, but none of those results was statistically significant.


```{r abs-perception-error-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  baseline_legend_of_use,
  standard_baseline_difference,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("|PERCEPTION ERROR|"),
  label = "tab:abs-perception-errors",
  title="Absolute perception error models",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  # float.env = "sidewaystable"
  )


```


## Dependent variable: ACCEPTANCE

```{r processing-acceptance, echo=FALSE, message = FALSE}
# NEED TO USE DATA WITHOUT DUPLICATES

acceptance_data_unique = responses_an %>%
    select(uuid, acceptance, legend) %>%
    distinct()

```

```{r mean-group-, echo=FALSE, message = FALSE}
# Generally by group, which highest
ordered_acceptance_criterias <- acceptance_data_unique %>%
  group_by(legend) %>%
  summarize(mean = mean(acceptance, na.rm = T)) %>%
  arrange(desc(mean))

ordered_acceptance_criterias$legend <- plyr::revalue(ordered_acceptance_criterias$legend, c(
  "headline"="Headline",
  "checkered"="Checkered",
  "sampled"="Sampled",
  "clustered"="Clustered",
  "annotated"="Annotated"
  ))


names(ordered_acceptance_criterias) <- c("Legend", "Mean")
ordered_acceptance_criterias$Legend <- as.character(ordered_acceptance_criterias$Legend)
ordered_acceptance_criterias$Mean <- round(ordered_acceptance_criterias$Mean, 2)


```



Table \@ref(tab:ordered-acceptance) shows the average *ACCEPTANCE*-score for each legend type. As expected, having no legend at all (Headline) have the lowest acceptance scores, while the Sampled and Annotated legend types have average acceptance scores above 4 out of a maximum of 5.


```{r processing-acceptance-out, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  ordered_acceptance_criterias,
  type='latex', 
  summary = FALSE, header = FALSE,
  label = "tab:ordered-acceptance",
  title="Average acceptance scores by legend type"
  )
```


```{r acceptance-data-processing-and-models, echo=FALSE, message = FALSE}

# Earlier baselines still makes rough sense
acceptance_data_unique_legend_baseline = acceptance_data_unique
acceptance_data_unique_legend_baseline$legend = relevel(acceptance_data_unique_legend_baseline$legend, ref = "headline")
acceptance_data_unique_checkered_baseline = acceptance_data_unique
acceptance_data_unique_checkered_baseline = acceptance_data_unique_checkered_baseline %>% filter(legend != "headline")
acceptance_data_unique_checkered_baseline$legend = relevel(acceptance_data_unique_checkered_baseline$legend, ref = "checkered")

# Models- all significant against no-legend (= preferred over)
acceptance_headline_baseline <- lm(acceptance ~ legend,
           data = acceptance_data_unique_legend_baseline)
# Annotated and sampled significantly preferred, clustered lower, but not significantly so
acceptance_checkered_baseline <- lm(acceptance ~ legend,
           data = acceptance_data_unique_checkered_baseline)

```

As seen from the regression results in Table \@ref(tab:acceptance-models), all acceptance means are statistically higher than the Headline category, and both Sampled and Annotated legend types are statistically significantly considered more helpful than the Clustered legend type by the respondents. The Clustered legend type has a lower acceptance score compared to the Checkered legend, but not statistically significantly so.


```{r acceptance-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  acceptance_headline_baseline,
  acceptance_checkered_baseline,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("ACCEPTANCE"),
  label = "tab:acceptance-models",
  title="Acceptance-score models",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```


## Robustness tests

The variables that were collected as potential proxy variables for uncertainty (*SUBMIT TIME*, *INPUT CHANGES*, and *HOVER EVENTS*) exhibited high variability and large outliers, likely due to external factors. Separate regression models against the legend types are shown in Tables \@ref(tab:models-time-to-submit), \@ref(tab:models-input-changes) and  \@ref(tab:models-hover-events) in Appendix A. *SUBMIT TIME* and *INPUT CHANGES* are significantly lower for the other legend types compared to Headline/no-legend. However, that was likely due to the first interaction with the UI was for the Headline type by design. At this point, many respondents did ask questions and changed their input continuously before submitting the answer. Comparison between other categories did not give any statistically significant differences.

To check for clear learning or fatigue effects that would merit subsample analysis, an OLS model was run on *|PERCEPTION ERROR|* against the linear progression variable (discrete values expressed as integers 1-10). Another model using only the subsample of the first and last images, i.e. all Headline type responses was estimated to check if the respondents learned to memorize the opacity data mapping without the use of legends.


```{r progression-model-checks, echo=FALSE, message = FALSE}

# Correlation error with progression
# learning effects as progression or getting tired of task... (INDIVIDUAL+PROGNUM)
progression_effect_mod <- lm(percept_error_abs ~ prog,
                             data = responses_an)
# Nothing significant, not going to take this into account - or do analysis using e.g. first 5 images only



# Baseline 1 vs baseline 10 - difference? - SUBSAMPLE ONLY 1st and 10th
progression_effect_mod_baseline_check <- lm(percept_error_abs ~ factor(prog),
                             data = responses_an %>% filter(prog %in% c(1, 10)))


```


The results can be found in Table \@ref(tab:progression-models). As seen, neither the linear trend variable nor the categorical variable for the last image in the progression was statistically significant. The estimate is even showing a slightly higher error for the last example on average compared to the first for the gathered sample of 68 (34 respondents, 2 examples per respondent).

```{r progression-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  progression_effect_mod,
  progression_effect_mod_baseline_check,
  header = FALSE,
  type='latex',
  covariate.labels=c("ProgressionLinear","Progression Last","Constant/Reference"),
  column.labels = c("Linear trend", "Subsample fist/last, Reference=First"),
  column.separate = c(1,1),
  dep.var.labels = c("|PERCEPTION ERROR|"),
  label = "tab:progression-models",
  title="Progression effects regression results",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```

These results indicate that conducting further analysis on e.g. a subsample of the first 5 examples in the progression for each respondent is of little value, as it would most likely only result in halving the sample size.


A subsample of the 50th percentile on average more accurate respondents was created to evaluate if this would remove large error outliers and give more significant estimates. Identical models as in section \ref{subsection-perception-error}, using *|PERCEPTION ERROR|* as dependent variable were estimated. 


```{r models-outlier-removal-preprocessing, echo=FALSE, message = FALSE}

# Respondents with smaller average errors (outlier removal) - top 50th percentile

num_respondents_chosen = (responses_an$uuid %>% unique() %>% length())/2

arranged_respondents_by_error = responses_an %>% group_by(uuid) %>%
  summarize(mean_abs_error = mean(percept_error_abs)) %>%
  arrange(mean_abs_error)

resp_low_errors = arranged_respondents_by_error[1:num_respondents_chosen,] %>% select(uuid)

resp_low_errors_df = responses_an %>% filter(uuid %in% resp_low_errors$uuid)
responses_an_accurate_viz = responses_an_viz %>% filter(uuid %in% resp_low_errors$uuid)

```

Visual inspection based on Figure \@ref(fig:models-outlier-removal-plot) shows that there is still a large variance in the distribution. The Headline legend type still has the highest average absolute error. However, the Clustered, and not the Checkered legend design, has the second-highest average absolute error for the subsample.

```{r models-outlier-removal-plot, out.width = '55%', fig.align = "center", fig.cap = "Absolute error - 50th percentile most accurate respondent subset", echo=FALSE, message = FALSE}
# Absolute
# Not really anything that sticks out
ggplot(responses_an_accurate_viz, aes(legend, percept_error_abs, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
    stat_summary(fun.data="mean_sdl", fun.args = list(mult = 1), 
               geom="pointrange", color="black") + 
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) +
  ylab("Absolute Perception Error") +
  xlab("Legend")


```


```{r models-outlier-removal-model-definition, echo=FALSE, message = FALSE}

# For completeness
no_legend_baseline_accurate = resp_low_errors_df
no_legend_baseline_accurate$legend = relevel(no_legend_baseline_accurate$legend, ref = "headline")
checkered_baseline_accurate = no_legend_baseline_accurate
checkered_baseline_accurate = checkered_baseline_accurate %>% filter(legend != "headline")
checkered_baseline_accurate$legend = relevel(checkered_baseline_accurate$legend, ref = "checkered")

# Absolute error - simple OLS
## 1. legend useful at all
### Baseline
baseline_legend_of_use_accurate <- lm(percept_error_abs ~ legend,
                             data = no_legend_baseline_accurate)
# summary(baseline_legend_of_use_accurate)


## 2. Legend vs standard = checkered
## 3. annotated or contextualized also answered
### Baseline
standard_baseline_difference_accurate <- lm(percept_error_abs ~ legend,
                                   data = checkered_baseline_accurate)

```

As seen in Table \@ref(tab:abs-perception-errors-accurate), which corresponds to Table \@ref(tab:abs-perception-errors) for the full sample, all estimates except for Clustered still have lower absolute errors compared to the Headline category that are statistically significant. For comparison, Checkered is still kept as a reference category in the models where Headline types have been excluded from the sample. No statistically significant results were estimated for model (2), as was the case when using the full sample.

```{r models-outlier-removal-models-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  baseline_legend_of_use_accurate,
  standard_baseline_difference_accurate,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("|PERCEPTION ERROR|"),
  label = "tab:abs-perception-errors-accurate",
  title="Absolute perception error models - above 50th percentile accuracy subsample",
  no.space = TRUE,
  single.row = TRUE,
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  # float.env = "sidewaystable"
  )


```


# Discussion 


Both the Bartlett's test results and the dummy-variable OLS models are consistent in establishing that for visualizations using transparency mapped overlays on top of a static base map, there is an influence of legend choices - as to if legends are included or not. It is evident that there is both a lower variance of errors in estimations from the respondents and more accurate results when there is a legend compared to when no legend (Headline type) is present. In other words, a legend improves the ability to make visual comparisons and reduces estimation errors (answering **RQ1**). 

There are no clear results indicating differences in response accuracy depending on the type/design of legend, and there is quite a large variation in response accuracy in the sample overall. These results indicate that the decoding support from a legend helps create a reference point for the user, but that it is not very precise or sensitive to the design of the legend used  (**RQ2**). These results for **RQ1** and **RQ2** were robust to subsamples of respondents with average lower errors. There are no indications that there are learning effects where the users are able to memorize the opacity to value mappings without making use of the legends, meaning that it ought to be safe to use the full sample.

**TODO: CONNECT PARAGRAPHS ABOVE TO EARLIER STUDIES OR PARTS OF THE STUDY**

The most robust results were from the user preferences/acceptance of the different legend types (**RQ3**), where both Sampled and Annotated (more contextualized and higher placement proximity to data) were preferred to the Checkered legend type ("industry standard" imitation). 

For the Annotated legend design type, this may be due to the reduced need for "attention switching" [@Kern10] following higher proximity to the data and marker. Another factor that is relevant for both Sampled and Annotated designs is that there may be higher familiarity for the user, as they have the background of the map superimposed for both these legend types.

Notably as well was that the number of times the users changed their responses before submitting a response, or the duration until the users submitted their responses from the visualization rendering was not well-suited to act as proxy variables for respondent confidence or uncertainty (not being possible to draw any valid conclusions to in the study to **RQ4**). 

During the data collection this may have been due to different respondents making use of the computer and UI-input elements in vastly different ways, and sometimes being distracted from the task or asking clarifying questions.

This resulted in that it was not possible to draw conclusions from the decision times and user selection behaviour for different legend designs. For future studies, this could likely be improved by using eye-tracking technology, where you could track that the duration the user is actively looking at the map, instead of trying to make use of proxy variables. This method has been used extensively in map design research [@Dong14; @Brychtova16; @Çöltekin09].

Regarding the general design of the study, it would be of interest to see how the results would differ when using different map backgrounds. Another variation that is common in use and has not been tested is having discrete (non-continuous) opacity-mapped data - i.e. where larger regions have the same opacity value. Users ability to perceive the opacity mapping where they have larger areas with the same value may differ from when there is pixel-level variation. This would also result in legends where the opacity values are binned, not showing the whole linear scale. 

The sampling was performed using non-random convenience sampling. As outlined in section \ref{collection-info}, this was chosen to keep external factors, such as screen size and lighting, as consistent as possible. Due to access to a suitable group of technically adept map users and practitioners, the results ought to be representative of how adept map users can utilize different legend designs. Future studies could investigate legends for users with less experience of mapping products, as no claims can be made that certain legend designs are helpful for users that don't have prior exposure to common map elements.

In using only one colour for the data opacity layer for each example, the effects of colour scales being somewhat arbitrary, as described by @Ware20 did not need to be investigated. It is, however, common in visualizations with a lower opacity to have either a constant opacity for a polygon, but using a colour scale to encode values, and it is also possible to have double encoding and have the same variable displayed using both opacity variations and colour encodings simultaneously. For this study, it would have been difficult to isolate and measure the effect of transparency variations with those designs.

The different colours used for the examples (red, green and blue) did not result in differing decoding accuracy. One or more of the participants were likely to have some kind of colour blindness, as most of the participants were men\footnote{*"Globally, about 1 in 12 men (8%) and 1 in 200 (0.5%) women sees color differently
from most of the population"* [@Lee2020]}. In this study, it is unclear if that would affect the difficulty of the decoding task, or increase issues with blending for specific colours. The respondents were not asked to distinguish between elements of different colours, but to evaluate opacity on top of a base map, where the combined colour blending results in general trends of contrast. If any systematic biases would exist for any colours, such effects may have become obvious from accuracy differences for different colours. However, that would likely require a much larger sample.

Relating to @Seipel20, where the task was to distinguish objects there ought to be less complication from colour blending in this study, due to only one lower opacity data layer on top of a fully opaque base layer. With the data encoded layers, it is possible to distinguish trends over a larger area of the base map. The opacity layer is still blended with the base map. Hence, in lower opacity areas, you would get more blending with the base layer, which may affect precision in the estimation from the respondents. This may have created less precision in some estimations, but is a natural effect of using opacity/transparency as an encoding mechanism. There could be certain colour combinations from the base map that makes the decoding task more difficult, which would be of interest to evaluate in future studies. In this study, those effects ought to have been smoothed out by randomization, and not have biased the results in any particular direction. 

In comparison to @Kiik17, the usage of only a single opacity layer did circumvent the issue of having blending from multiple low opacity layers. 

In the larger literature on data visualization, there have been discussions on the usefulness of legends in general. Alternatives such as simplifying the visualizations, directly labelling the data, altering visual weight and making use of motion to remove the need for legends have been suggested [@Evergreen13]. 

In the field of cartography, legends containing map symbols are commonly listed as one of the essential elements of a map together with title, scale bar, north arrow and a few other elements [@Peterson09, p. 17]. This study showed that legends can effectively increase decoding accuracy for complex visualizations (**RQ1**), where the techniques described by @Evergreen13 may not be easy to implement. 

The legend design variations created for this study to investigate a reduced need for "attention switching" through data-proximity from the legend [@Kern10; @Evergreen13] did not result in significantly higher decoding accuracy (**RQ2**), but was one of the designs preferred by the respondents (**RQ3**).



# Conclusions

When using transparency mapping, there were strong and statistically significant results that map designs using legends provide reduced visual estimation errors (**RQ1**). However, there were no statistically differing errors when comparing the different legend designs created for this study (**RQ2**). In terms of user preference/acceptance of the different legend designs, both Sampled and Annotated designs were preferred (**RQ3**). These were also the two designs that had the overall lowest estimation errors. The duration until the users submitted responses or the number of times they changed their response before submitting were influenced by too many external factors to provide useful inference to respondent confidence or uncertainty (**RQ4**).






\newpage

# References

<div id="refs"></div>


\newpage

\section{Appendices} \label{appendices}

\subsection{Appendix A - plots and tables}

**Note:** All analysis code and data for reproducing results can be found at https://github.com/Tille88/thesis-data-analysis


```{r submit-time-visualization, fig.cap = "Submit time across legend types", echo=FALSE}

# Submit time
ggplot(responses_an_viz, aes(legend, submitTime / 1000, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) +
  ylab("Seconds until Submit") +
  xlab("Legend")

```

```{r input-changes-visualization, fig.cap = "Input changes across legend types", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, inputChanges, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) +
  ylab("Number of Input Changes before Submit") +
  xlab("Legend")

```


```{r hover-events-visualization, fig.cap = "Hover events across legend types", echo=FALSE}

# Input changes
ggplot(responses_an_viz, aes(legend, hoverEvents, color=legend)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.3) +
  theme(legend.position="none") +
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  ) +
  ylab("Number of Hover Events before Submit") +
  xlab("Legend")

```




```{r colour-visualization, fig.cap = "Absolute perception error by colour types", echo=FALSE}


ggplot(responses_an_viz, aes(colour, percept_error_abs, color=colour)) +
  geom_violin() + geom_jitter(height = 0, width = 0.1, alpha=0.5) +
  scale_color_manual(values=c("blue", "green", "red")) +
  stat_summary(fun="mean", geom="point", color="black") +
  ylab("Absolute Perception Error")+
  xlab("Colour") +
  theme_minimal() +
  theme(
    legend.position="none",
    plot.background = element_rect(colour = "black", fill=NA, size=1)
  )
  

```


```{r interaction-variable-modelling, echo=FALSE, message = FALSE}

# As shown earlier - a lot of variance, not a lot of signal
# Outliers likely affecting, simple OLS only

# Time until submit -> likely because of first interaction as legend
time_submit_all <- lm(submitTime / 1000 ~ legend,
                             data = no_legend_baseline)
# Between those with legend, no statistically significant difference difference compared to baseline
time_submit_w_legend <- lm(submitTime / 1000 ~ legend,
                  data = checkered_baseline)


# inputChanges - only on first interaction, nothing significant vs. checkered
input_changes_all <- lm(inputChanges ~ legend,
           data = no_legend_baseline)
input_changes_w_legend <- lm(inputChanges ~ legend,
           data = checkered_baseline)

# hoverEvents - no significant difference, high std errors, order of baseline category will not matter
hover_events_all <- lm(hoverEvents ~ legend,
           data = no_legend_baseline)
hover_events_w_legend <- lm(hoverEvents ~ legend,
           data = checkered_baseline)

```

```{r time-submit-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  time_submit_all,
  time_submit_w_legend,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("SUBMIT TIME"),
  label = "tab:models-time-to-submit",
  title="Time to submit regression results",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```


```{r input-changes-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  input_changes_all,
  input_changes_w_legend,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("INPUT CHANGES"),
  label = "tab:models-input-changes",
  title="Input changes regression results",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```

```{r hover-events-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  hover_events_all,
  hover_events_w_legend,
  header = FALSE,
  type='latex',
  covariate.labels=c("Annotated","Checkered","Clustered",
  "Sampled","Constant/Reference"),
  column.labels = c("Headline Reference", "Checkered Reference"),
  column.separate = c(1,1),
  dep.var.labels = c("HOVER EVENTS"),
  label = "tab:models-hover-events",
  title="Hover events regression results",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```


```{r color-modeling, echo=FALSE, message = FALSE}

colour_model <- lm(percept_error_abs ~ no_legend_baseline$colour,
                             data = no_legend_baseline)

```


```{r color-model-table, echo=FALSE, message = FALSE, results='asis'}
stargazer(
  colour_model,
  header = FALSE,
  type='latex',
  covariate.labels=c("Green","Red","Constant/Reference = Blue"),
  column.labels = c("Full sample"),
  dep.var.labels = c("|PERCEPTION ERROR|"),
  label = "tab:models-color",
  title="Absolute perception error by colour",
  omit.stat = c("adj.rsq", "rsq", "f", "ser")
  )

```








